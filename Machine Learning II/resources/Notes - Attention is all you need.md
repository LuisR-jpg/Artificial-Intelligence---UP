# Attention is all you need

Google's proposal from 2017. It overpased state of the art with less execution time.

This proposal is called *Transformer* and it's based on attention mechanisms.

## Background

### Self-attention

Self-attention, sometimes called intra-attention is an attention mechanism relating different positions
of a single sequence in order to compute a representation of the sequence. Self-attention has been
used successfully in a variety of tasks including reading comprehension, abstractive summarization,
textual entailment and learning task-independent sentence representations [4, 27, 28, 22].

