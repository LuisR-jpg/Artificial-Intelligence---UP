{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from tqdm.auto import tqdm\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pad_packed_sequence, pack_padded_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agression_dataset(Dataset):\n",
    "    def __init__(self, split):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.load_data(split)\n",
    "        self.vocab, self.emb_mat = self.load_vocab_embeddings()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        '''Método principal para cargar una observación del dataset.\n",
    "           label: categoría a la que pertenece la observación.\n",
    "           words_ids: lista de índices de las palabras en el vocabulario.\n",
    "        '''\n",
    "        label = self.data.iloc[index]['target']\n",
    "        words, words_ids = self.preprocessed_text(index)\n",
    "        return words_ids, label, words\n",
    "    \n",
    "    def preprocessed_text(self, index):\n",
    "        '''Preprocess text and '''\n",
    "\n",
    "        text = self.data.iloc[index]['text']\n",
    "        words = nltk.word_tokenize(text)\n",
    "        words_ids = [self.vocab[word] if word in self.vocab.keys() else self.emb_mat.shape[0]-1\\\n",
    "                        for word in words]\n",
    "        return words, words_ids\n",
    "    \n",
    "    def load_data(self, split):\n",
    "        '''Método para cargar los datos.\n",
    "           El texto está en la columna \"text\" y las categorías en la columna \"target\".        \n",
    "        '''\n",
    "        self.data = pd.read_csv('%s.csv'%(split))\n",
    "\n",
    "    def load_vocab_embeddings(self):\n",
    "        '''Embeddings preentrenados en twitter.\n",
    "           emb_mat: Matriz de embeddings. Un vector de tamaño 200 para cada palabra del vocabulario.\n",
    "           vocab: Diccionario, asigna a cada palabra su renglón correspondiente en la matriz de embeddings.        \n",
    "        '''\n",
    "        embeddings_list = []\n",
    "        self.vocab_dict = {}\n",
    "        vocab = {}\n",
    "        with open('word2vec_col.txt', 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i!=0:\n",
    "                    values = line.split()\n",
    "                    self.vocab_dict[i+1] = values[0]\n",
    "                    vocab[values[0]] = i+1\n",
    "                    vector = np.asarray(values[1:], 'float32')\n",
    "                    embeddings_list.append(vector)\n",
    "        embeddings_list.insert(0,np.mean(np.vstack(embeddings_list), axis=0))\n",
    "        embeddings_list.insert(0,np.zeros(100))\n",
    "        self.vocab_dict[0] = '[PAD]'\n",
    "        self.vocab_dict[1] = '[UNK]'\n",
    "        vocab['[PAD]'] = 0\n",
    "        vocab['[UNK]'] = 1\n",
    "        emb_mat = np.vstack(embeddings_list)\n",
    "\n",
    "        return vocab, emb_mat\n",
    "\n",
    "    def get_weights(self):\n",
    "        '''Devuelve pesos inversos para cada categoría. Mayor peso para la categoría con menos observaciones.'''\n",
    "        \n",
    "        cat_0 = len(self.data[self.data['target']==0])\n",
    "        cat_1 = len(self.data[self.data['target']==1])\n",
    "        maxi = max(cat_0, cat_1)\n",
    "        return torch.tensor([maxi/cat_0, maxi/cat_1])\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        '''Función que ejecuta el dataloader para formar batches de datos.'''\n",
    "        zipped_batch = list(zip(*batch))\n",
    "        word_ids = [torch.tensor(t) for t in zipped_batch[0]]\n",
    "        word_ids = torch.cat(word_ids, dim=0)\n",
    "        lengths = torch.tensor([len(t) for t in zipped_batch[0]])\n",
    "        labels = torch.tensor(zipped_batch[1])\n",
    "        words = zipped_batch[2]\n",
    "        return word_ids, lengths, labels, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size=100, hidden_size=128, num_layers=1, \n",
    "                 bidirectional=False, emb_mat=None, dense_hidden_size=256):\n",
    "        '''Constructor, aquí definimos las capas.\n",
    "        input:\n",
    "            input_size: Tamaño de los embeddings de las palabras.\n",
    "            hidden_size: Tamaño de la capa oculta de la GRU. \n",
    "            num_layers: Número de capas de la GRU.\n",
    "            bidirectional: True si se quiere una GRU bidireccional. \n",
    "            emb_mat: Matriz de embeddings del vocabulario.\n",
    "            dense_hidden_size: Tamaño de la capa ocula del clasificador.\n",
    "        '''\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        # Matriz entrenable de embeddings, tamaño del vocab_size x 100\n",
    "        self.embeddings = nn.Embedding.from_pretrained(\\\n",
    "                            torch.FloatTensor(emb_mat), freeze=False)\n",
    "        # Gated Recurrent Unit\n",
    "        self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, bidirectional=bidirectional)\n",
    "        # Número de direcciones de la GRU\n",
    "        directions = 2 if bidirectional else 1\n",
    "        # Clasificador MLP\n",
    "        self.classifier = nn.Sequential(\\\n",
    "                            nn.Linear(hidden_size*directions, dense_hidden_size),\n",
    "                            nn.BatchNorm1d(dense_hidden_size),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(dense_hidden_size, 2))\n",
    "        \n",
    "    def forward(self, input_seq, lengths):\n",
    "        '''Función feed-forward de la red.\n",
    "        input:\n",
    "            input_seq: Lista de ids para cada palabra.\n",
    "            lengths: Número de palabras en cada una de las observaciones del batch.\n",
    "        output:\n",
    "            x: vectores para clasificar.\n",
    "            return None for consistency with the next model\n",
    "        '''\n",
    "        # Calcula el embedding para cada palabra.\n",
    "        x = self.embeddings(input_seq)\n",
    "        # Forma las secuencias de palabras que entran a la GRU.\n",
    "        x = x.split(lengths.tolist())\n",
    "        # Añade pading y empaqueta las secuencias (mayor velocidad de cómputo).\n",
    "        x = pad_sequence(x)\n",
    "        x = pack_padded_sequence(x, lengths, enforce_sorted=False)\n",
    "        output, hn = self.gru(x)\n",
    "        hn = torch.cat([h for h in hn], dim=-1)\n",
    "        x = self.classifier(hn)\n",
    "        return x, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, dataloader, criterion, device):\n",
    "    '''Función para evaluar el modelo.'''\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        losses = []\n",
    "        preds = torch.empty(0).long()\n",
    "        targets = torch.empty(0).long()\n",
    "        scores_list = []\n",
    "        words_list = []\n",
    "        pred_list = []\n",
    "        for data in tqdm(dataloader):\n",
    "            torch.cuda.empty_cache()\n",
    "            seq, seq_len, labels, words = data \n",
    "            seq, labels = seq.to(device), labels.to(device)\n",
    "            output, scores = model(seq, seq_len)\n",
    "            output = F.log_softmax(output, dim=1)\n",
    "            loss = criterion(output, labels)\n",
    "            losses.append(loss.item())\n",
    "            predictions = F.log_softmax(output, dim=1).argmax(1)\n",
    "            preds = torch.cat([preds, predictions.cpu()],dim=0)\n",
    "            targets = torch.cat([targets, labels.cpu()],dim=0)\n",
    "            if scores is not None:\n",
    "                pred_list += predictions.tolist()\n",
    "                scores = scores.cpu().squeeze(2).tolist()\n",
    "                scores_list += scores \n",
    "                words_list += words \n",
    "\n",
    "        model.train()\n",
    "        preds = preds.numpy()\n",
    "        targets = targets.numpy()\n",
    "        f1 = f1_score(targets, preds, average='binary')\n",
    "\n",
    "        return np.mean(losses), f1, scores_list, words_list, pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = agression_dataset('train')\n",
    "val_dataset = agression_dataset('val')\n",
    "test_dataset = agression_dataset('test')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn = train_dataset.collate_fn, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, collate_fn = val_dataset.collate_fn, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn = test_dataset.collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "epochs = 10\n",
    "weight_decay=0.0001\n",
    "beta1=0\n",
    "beta2=0.999\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleRNN(emb_mat=train_dataset.emb_mat, bidirectional=False).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr,weight_decay=weight_decay, betas = (beta1, beta2))\n",
    "weight = train_dataset.get_weights().to(device)\n",
    "criterion = nn.NLLLoss(weight = weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_f1 = 0\n",
    "for epoch in range(epochs):\n",
    "    for data in tqdm(train_dataloader):\n",
    "        # Limpia basura de la memoria GPU\n",
    "        torch.cuda.empty_cache()\n",
    "        # Reiniciamos el cálculo del gradiente\n",
    "        optimizer.zero_grad()\n",
    "        # Desempaca los datos que salen del dataloader\n",
    "        seq, seq_len, labels, _ = data \n",
    "        # Mueve los datos al mismo device en el que esté el modelo\n",
    "        seq, labels = seq.to(device), labels.to(device)\n",
    "        output, _ = model(seq, seq_len)\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        loss = criterion(output, labels)\n",
    "        # Calcula el gradiente de la pérdida\n",
    "        loss.backward()\n",
    "        # Realiza un paso de la optimización\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evalúa los modelos en los conjuntos de entrenamiento y validación\n",
    "    train_loss, train_f1, _, _, _ = eval_model(model, train_dataloader, criterion, device)\n",
    "    val_loss, val_f1, _, _, _ = eval_model(model, val_dataloader, criterion, device)\n",
    "    print('epoch: %d'%(epoch))\n",
    "    print('train_loss: %5f | val_loss: %5f | train_f1: %5f | val_f1: %5f'%(train_loss, val_loss, train_f1, val_f1))\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1=val_f1\n",
    "        best_state_dict=copy.deepcopy(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(best_state_dict)\n",
    "train_loss, train_f1, _, _, _ = eval_model(model, train_dataloader, criterion, device)\n",
    "val_loss, val_f1, _, _, _ = eval_model(model, val_dataloader, criterion, device)\n",
    "test_loss, test_f1, _, _, _ = eval_model(model, test_dataloader, criterion, device)\n",
    "print('train_loss: %5f | train_f1: %5f'%(train_loss, train_f1))\n",
    "print('val_loss: %5f | val_f1: %5f'%(val_loss, val_f1))\n",
    "print('test_loss: %5f | test_f1: %5f'%(test_loss, test_f1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
