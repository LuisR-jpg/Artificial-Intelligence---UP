{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from tqdm.auto import tqdm\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pad_packed_sequence, pack_padded_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agression_dataset(Dataset):\n",
    "    def __init__(self, split):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.load_data(split)\n",
    "        self.vocab, self.emb_mat = self.load_vocab_embeddings()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        '''Método principal para cargar una observación del dataset.\n",
    "           label: categoría a la que pertenece la observación.\n",
    "           words_ids: lista de índices de las palabras en el vocabulario.\n",
    "        '''\n",
    "        label = self.data.iloc[index]['target']\n",
    "        words, words_ids = self.preprocessed_text(index)\n",
    "        return words_ids, label, words\n",
    "    \n",
    "    def preprocessed_text(self, index):\n",
    "        '''Preprocess text and '''\n",
    "\n",
    "        text = self.data.iloc[index]['text']\n",
    "        words = nltk.word_tokenize(text)\n",
    "        words_ids = [self.vocab[word] if word in self.vocab.keys() else self.emb_mat.shape[0]-1\\\n",
    "                        for word in words]\n",
    "        return words, words_ids\n",
    "    \n",
    "    def load_data(self, split):\n",
    "        '''Método para cargar los datos.\n",
    "           El texto está en la columna \"text\" y las categorías en la columna \"target\".        \n",
    "        '''\n",
    "        self.data = pd.read_csv('%s.csv'%(split))\n",
    "\n",
    "    def load_vocab_embeddings(self):\n",
    "        '''Embeddings preentrenados en twitter.\n",
    "           emb_mat: Matriz de embeddings. Un vector de tamaño 200 para cada palabra del vocabulario.\n",
    "           vocab: Diccionario, asigna a cada palabra su renglón correspondiente en la matriz de embeddings.        \n",
    "        '''\n",
    "        embeddings_list = []\n",
    "        self.vocab_dict = {}\n",
    "        vocab = {}\n",
    "        prefix = '../../data/'\n",
    "        with open(prefix + 'word2vec_col.txt', 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i!=0:\n",
    "                    values = line.split()\n",
    "                    self.vocab_dict[i+1] = values[0]\n",
    "                    vocab[values[0]] = i+1\n",
    "                    vector = np.asarray(values[1:], 'float32')\n",
    "                    embeddings_list.append(vector)\n",
    "        embeddings_list.insert(0,np.mean(np.vstack(embeddings_list), axis=0))\n",
    "        embeddings_list.insert(0,np.zeros(100))\n",
    "        self.vocab_dict[0] = '[PAD]'\n",
    "        self.vocab_dict[1] = '[UNK]'\n",
    "        vocab['[PAD]'] = 0\n",
    "        vocab['[UNK]'] = 1\n",
    "        emb_mat = np.vstack(embeddings_list)\n",
    "\n",
    "        return vocab, emb_mat\n",
    "\n",
    "    def get_weights(self):\n",
    "        '''Devuelve pesos inversos para cada categoría. Mayor peso para la categoría con menos observaciones.'''\n",
    "        \n",
    "        cat_0 = len(self.data[self.data['target']==0])\n",
    "        cat_1 = len(self.data[self.data['target']==1])\n",
    "        maxi = max(cat_0, cat_1)\n",
    "        return torch.tensor([maxi/cat_0, maxi/cat_1])\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        '''Función que ejecuta el dataloader para formar batches de datos.'''\n",
    "        zipped_batch = list(zip(*batch))\n",
    "        word_ids = [torch.tensor(t) for t in zipped_batch[0]]\n",
    "        word_ids = torch.cat(word_ids, dim=0)\n",
    "        lengths = torch.tensor([len(t) for t in zipped_batch[0]])\n",
    "        labels = torch.tensor(zipped_batch[1])\n",
    "        words = zipped_batch[2]\n",
    "        return word_ids, lengths, labels, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size=100, hidden_size=128, num_layers=1, \n",
    "                 bidirectional=False, emb_mat=None, dense_hidden_size=256):\n",
    "        '''Constructor, aquí definimos las capas.\n",
    "        input:\n",
    "            input_size: Tamaño de los embeddings de las palabras.\n",
    "            hidden_size: Tamaño de la capa oculta de la GRU. \n",
    "            num_layers: Número de capas de la GRU.\n",
    "            bidirectional: True si se quiere una GRU bidireccional. \n",
    "            emb_mat: Matriz de embeddings del vocabulario.\n",
    "            dense_hidden_size: Tamaño de la capa ocula del clasificador.\n",
    "        '''\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        # Matriz entrenable de embeddings, tamaño del vocab_size x 100\n",
    "        self.embeddings = nn.Embedding.from_pretrained(\\\n",
    "                            torch.FloatTensor(emb_mat), freeze=False)\n",
    "        # Gated Recurrent Unit\n",
    "        self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, bidirectional=bidirectional)\n",
    "        # Número de direcciones de la GRU\n",
    "        directions = 2 if bidirectional else 1\n",
    "        # Clasificador MLP\n",
    "        self.classifier = nn.Sequential(\\\n",
    "                            nn.Linear(hidden_size*directions, dense_hidden_size),\n",
    "                            nn.BatchNorm1d(dense_hidden_size),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(dense_hidden_size, 2))\n",
    "        \n",
    "    def forward(self, input_seq, lengths):\n",
    "        '''Función feed-forward de la red.\n",
    "        input:\n",
    "            input_seq: Lista de ids para cada palabra.\n",
    "            lengths: Número de palabras en cada una de las observaciones del batch.\n",
    "        output:\n",
    "            x: vectores para clasificar.\n",
    "            return None for consistency with the next model\n",
    "        '''\n",
    "        # Calcula el embedding para cada palabra.\n",
    "        x = self.embeddings(input_seq)\n",
    "        # Forma las secuencias de palabras que entran a la GRU.\n",
    "        x = x.split(lengths.tolist())\n",
    "        # Añade pading y empaqueta las secuencias (mayor velocidad de cómputo).\n",
    "        x = pad_sequence(x)\n",
    "        x = pack_padded_sequence(x, lengths, enforce_sorted=False)\n",
    "        output, hn = self.gru(x)\n",
    "        hn = torch.cat([h for h in hn], dim=-1)\n",
    "        x = self.classifier(hn)\n",
    "        return x, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, dataloader, criterion, device):\n",
    "    '''Función para evaluar el modelo.'''\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        losses = []\n",
    "        preds = torch.empty(0).long()\n",
    "        targets = torch.empty(0).long()\n",
    "        scores_list = []\n",
    "        words_list = []\n",
    "        pred_list = []\n",
    "        for data in tqdm(dataloader):\n",
    "            torch.cuda.empty_cache()\n",
    "            seq, seq_len, labels, words = data \n",
    "            seq, labels = seq.to(device), labels.to(device)\n",
    "            output, scores = model(seq, seq_len)\n",
    "            output = F.log_softmax(output, dim=1)\n",
    "            loss = criterion(output, labels)\n",
    "            losses.append(loss.item())\n",
    "            predictions = F.log_softmax(output, dim=1).argmax(1)\n",
    "            preds = torch.cat([preds, predictions.cpu()],dim=0)\n",
    "            targets = torch.cat([targets, labels.cpu()],dim=0)\n",
    "            if scores is not None:\n",
    "                pred_list += predictions.tolist()\n",
    "                scores = scores.cpu().squeeze(2).tolist()\n",
    "                scores_list += scores \n",
    "                words_list += words \n",
    "\n",
    "        model.train()\n",
    "        preds = preds.numpy()\n",
    "        targets = targets.numpy()\n",
    "        f1 = f1_score(targets, preds, average='binary')\n",
    "\n",
    "        return np.mean(losses), f1, scores_list, words_list, pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '../../data/agresividad/'\n",
    "train_dataset = agression_dataset(prefix + 'train')\n",
    "val_dataset = agression_dataset(prefix + 'val')\n",
    "test_dataset = agression_dataset(prefix + 'test')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn = train_dataset.collate_fn, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, collate_fn = val_dataset.collate_fn, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn = test_dataset.collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "epochs = 10\n",
    "weight_decay=0.0001\n",
    "beta1=0\n",
    "beta2=0.999\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleRNN(emb_mat=train_dataset.emb_mat, bidirectional=False).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr,weight_decay=weight_decay, betas = (beta1, beta2))\n",
    "weight = train_dataset.get_weights().to(device)\n",
    "criterion = nn.NLLLoss(weight = weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_f1 = 0\n",
    "for epoch in range(epochs):\n",
    "    for data in tqdm(train_dataloader):\n",
    "        # Limpia basura de la memoria GPU\n",
    "        torch.cuda.empty_cache()\n",
    "        # Reiniciamos el cálculo del gradiente\n",
    "        optimizer.zero_grad()\n",
    "        # Desempaca los datos que salen del dataloader\n",
    "        seq, seq_len, labels, _ = data \n",
    "        # Mueve los datos al mismo device en el que esté el modelo\n",
    "        seq, labels = seq.to(device), labels.to(device)\n",
    "        output, _ = model(seq, seq_len)\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        loss = criterion(output, labels)\n",
    "        # Calcula el gradiente de la pérdida\n",
    "        loss.backward()\n",
    "        # Realiza un paso de la optimización\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evalúa los modelos en los conjuntos de entrenamiento y validación\n",
    "    train_loss, train_f1, _, _, _ = eval_model(model, train_dataloader, criterion, device)\n",
    "    val_loss, val_f1, _, _, _ = eval_model(model, val_dataloader, criterion, device)\n",
    "    print('epoch: %d'%(epoch))\n",
    "    print('train_loss: %5f | val_loss: %5f | train_f1: %5f | val_f1: %5f'%(train_loss, val_loss, train_f1, val_f1))\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1=val_f1\n",
    "        best_state_dict=copy.deepcopy(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(best_state_dict)\n",
    "train_loss, train_f1, _, _, _ = eval_model(model, train_dataloader, criterion, device)\n",
    "val_loss, val_f1, _, _, _ = eval_model(model, val_dataloader, criterion, device)\n",
    "test_loss, test_f1, _, _, _ = eval_model(model, test_dataloader, criterion, device)\n",
    "print('train_loss: %5f | train_f1: %5f'%(train_loss, train_f1))\n",
    "print('val_loss: %5f | val_f1: %5f'%(val_loss, val_f1))\n",
    "print('test_loss: %5f | test_f1: %5f'%(test_loss, test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention\n",
    "\n",
    "La sintaxis del modelo es similar al anterior pero se anade un modulo de atencion. El modulo de atencion toma los vectores de salida $h_t$ de la GRU y calcula una representacion $s$ como suma ponderada.\n",
    "\n",
    "$$ s = \\sum_t \\alpha_t h_t $$\n",
    "\n",
    "donde\n",
    "\n",
    "$$ u_t = tanh(Wh_t + b), $$\n",
    "\n",
    "$$ \\alpha_t = \\frac{exp(u_t^T)}{\\sum_i exp(u_i^T u)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnModule(nn.Module):\n",
    "    def __init__(self, input_size, attn_hidden_size=128):\n",
    "        '''\n",
    "        input:\n",
    "            input_size: tamano de la capa oculta del GRU. \n",
    "            attn_hidden_size: tamaño de la capa oculta.\n",
    "        '''\n",
    "        super(AttnModule, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, attn_hidden_size)\n",
    "        self.fc2 = nn.Linear(attn_hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, seq, lengths):\n",
    "        '''\n",
    "        input:\n",
    "            seq: secuencia de vectores ocultos de la GRU.\n",
    "            lengths: numero de palabras en cada observacion.\n",
    "        '''\n",
    "        x = pad_packed_sequence(seq)[0]\n",
    "        seq_len, batch_size, nhid = x.size()\n",
    "        u = self.fc1(x.view(batch_size*seq_len, nhid))\n",
    "        u = torch.tanh(u)\n",
    "        scores = self.fc2(u)\n",
    "        scores = scores.view(seq_len, batch_size, 1)\n",
    "        #Asigna -100 a las posiciones con padding para que no sean considerados en la atencion.\n",
    "        scores = nn.utils.rnn.pack_padded_sequence(scores, lengths=lengths,enforce_sorted=False)\n",
    "        scores = nn.utils.rnn.pad_packed_sequence(scores, padding_value=-100)[0]\n",
    "        scores = F.softmax(scores, dim=0)\n",
    "        scores = scores.transpose(0,1)\n",
    "        x = x.transpose(0,1).transpose(1,2)\n",
    "        x = torch.bmm(x, scores)\n",
    "        return x.squeeze(2), scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnRNN(nn.Module):\n",
    "    def __init__(self, input_size=100, hidden_size=128, num_layers=1, \n",
    "                 bidirectional=False, emb_mat=None, dense_hidden_size=256,\n",
    "                 attn_hidden_size=128):\n",
    "        super(AttnRNN, self).__init__()\n",
    "        self.embeddings = nn.Embedding.from_pretrained(\\\n",
    "                            torch.FloatTensor(emb_mat), freeze=False)\n",
    "        self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, bidirectional=bidirectional)\n",
    "        directions = 2 if bidirectional else 1\n",
    "        self.attn = AttnModule(input_size=hidden_size*directions)\n",
    "        self.classifier = nn.Sequential(\\\n",
    "                            nn.Linear(hidden_size*directions, dense_hidden_size),\n",
    "                            nn.BatchNorm1d(dense_hidden_size),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(dense_hidden_size, 2))\n",
    "        \n",
    "    def forward(self, input_seq, lengths):\n",
    "        x = self.embeddings(input_seq)\n",
    "        x = x.split(lengths.tolist())\n",
    "        x = pad_sequence(x)\n",
    "        x = pack_padded_sequence(x, lengths, enforce_sorted=False)\n",
    "        output, hn = self.gru(x)\n",
    "        x, scores = self.attn(output, lengths)\n",
    "        x = self.classifier(x)\n",
    "        return x, scores.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "epochs = 20\n",
    "device = torch.device('cuda')\n",
    "weight_decay=0.0001\n",
    "beta1=0\n",
    "beta2=0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttnRNN(emb_mat=train_dataset.emb_mat, bidirectional=False).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr,weight_decay=weight_decay, betas = (beta1, beta2))\n",
    "weight = train_dataset.get_weights().to(device)\n",
    "criterion = nn.NLLLoss(weight = weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_f1 = 0\n",
    "for epoch in range(epochs):\n",
    "    for data in tqdm(train_dataloader):\n",
    "        torch.cuda.empty_cache()\n",
    "        optimizer.zero_grad()\n",
    "        seq, seq_len, labels, _ = data \n",
    "        seq, labels = seq.to(device), labels.to(device)\n",
    "        output, _ = model(seq, seq_len)\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss, train_f1, _, _, _ = eval_model(model, train_dataloader, criterion, device)\n",
    "    val_loss, val_f1, _, _, _ = eval_model(model, val_dataloader, criterion, device)\n",
    "    print('epoch: %d'%(epoch))\n",
    "    print('train_loss: %5f | val_loss: %5f | train_f1: %5f | val_f1: %5f'%(train_loss, val_loss, train_f1, val_f1))\n",
    "    if val_f1>best_val_f1:\n",
    "        best_val_f1=val_f1\n",
    "        best_state_dict=copy.deepcopy(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(best_state_dict)\n",
    "train_loss, train_f1, train_scores, train_words, train_pred = eval_model(model, train_dataloader, criterion, device)\n",
    "val_loss, val_f1, val_scores, val_words, val_pred = eval_model(model, val_dataloader, criterion, device)\n",
    "test_loss, test_f1, test_scores, test_words, test_pred = eval_model(model, test_dataloader, criterion, device)\n",
    "print('train_loss: %5f | train_f1: %5f'%(train_loss, train_f1))\n",
    "print('val_loss: %5f | val_f1: %5f'%(val_loss, val_f1))\n",
    "print('test_loss: %5f | test_f1: %5f'%(test_loss, test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizando la atencion\n",
    "\n",
    "Uno de los beneficios de los mecanismos de atencion es que nos permiten identificar que elementos de las oraciones resultan mas importantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorize(words, color_array):\n",
    "    '''\n",
    "        Funcion para visualizar la atencion, tomada de https://gist.github.com/ihsgnef/f13c35cd46624c8f458a4d23589ac768,\n",
    "    '''\n",
    "    # words is a list of words\n",
    "    # color_array is an array of numbers between 0 and 1 of length equal to words\n",
    "    cmap = matplotlib.cm.get_cmap('Reds')\n",
    "    template = '<span class=\"barcode\"; style=\"color: black; background-color: {}\">{}</span>'\n",
    "    colored_string = ''\n",
    "    for word, color in zip(words, color_array):\n",
    "        color = matplotlib.colors.rgb2hex(cmap(color)[:3])\n",
    "        colored_string += template.format(color, '&nbsp' + word + '&nbsp')\n",
    "    return colored_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las palabras con mas atencion se muestran en color azul y aquellas con menor atencion en color rojo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att = np.linspace(0,1,50)\n",
    "p = [' ']*50\n",
    "s = colorize(p, att)\n",
    "# to display in ipython notebook\n",
    "display(HTML(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_attn = [np.max(scores) for scores in train_scores]\n",
    "maxi = np.flip(np.argsort(max_attn))\n",
    "for j in range(30):\n",
    "    i = maxi[j]\n",
    "    s = colorize(train_words[i], train_scores[i][:len(train_words[i])])\n",
    "    # to display in ipython notebook\n",
    "    category = 'Agresivo' if train_pred[maxi[j]]==1 else 'no agresivo'\n",
    "    print('Categoría predicha: %s'%(category))\n",
    "    display(HTML(s))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
