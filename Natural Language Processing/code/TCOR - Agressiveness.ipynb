{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Luis Eduardo Robles Jiménez\n",
    "\n",
    "### Natural Language Processing\n",
    "\n",
    "### Practica 3: BoW y Esquemas de pesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts_from_file(path_corpus, path_truth):\n",
    "    txt = []\n",
    "    y = []\n",
    "    with open(path_corpus, \"r\") as f_corpus, open(path_truth, \"r\") as f_truth:\n",
    "        for tuit in f_corpus:\n",
    "            txt += [tuit]\n",
    "        for label in f_truth:\n",
    "            y += [label] \n",
    "    return txt, list(map(int, y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_txt, tr_y = get_texts_from_file(\"../data/agresividad/mex_train.txt\", \"../data/agresividad/mex_train_labels.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tr_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tr_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See dist of labels\n",
    "\n",
    "print(Counter(tr_y))\n",
    "\n",
    "plt.hist(tr_y, bins=len(set(tr_y)))\n",
    "plt.ylabel('Users')\n",
    "plt.xlabel('Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_txt[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split doesn't quite work because of punctuation\n",
    "\n",
    "set(tr_txt[5].split()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.tokenize import TweetTokenizer # Tokenizer for social networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizar un tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(tr_txt[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(\"Hola @nick como estas #felizdia bye!!! hola@\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(\"https://www.youtube.com/watch?v=dhhS_g78X2E @\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_palabras = []\n",
    "for doc in tr_txt:\n",
    "    corpus_palabras += tokenizer.tokenize(doc) # A single list\n",
    "    #corpus_palabras += [tokenizer.tokenize(doc)] # Creates a list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(corpus_palabras))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist(corpus_palabras) # Frequency of each word\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortFreqDist(freqdict):\n",
    "    aux = [(freqdict[key], key) for key in freqdict]\n",
    "    aux.sort()\n",
    "    aux.reverse()\n",
    "    return aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = sortFreqDist(fdist) # Vocabulario\n",
    "V = V[:5000]\n",
    "V[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_indices = dict()\n",
    "cont = 0\n",
    "for weight, word in V:\n",
    "    dict_indices[word] = cont\n",
    "    cont += 1\n",
    "print(len(dict_indices))\n",
    "list(dict_indices)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_txt, val_y = get_texts_from_file(\"../data/agresividad/mex_val.txt\", \"../data/agresividad/mex_val_labels.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See dist of labels\n",
    "print(Counter(val_y))\n",
    "\n",
    "plt.hist(val_y, bins=len(set(val_y)))\n",
    "plt.ylabel('Users')\n",
    "plt.xlabel('Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_binary_bow(tr_txt, V, dict_indices): #List of all tweets, vocabulary, ordered dict(word, freq)\n",
    "    BOW = np.zeros((len(tr_txt),len(V)), dtype = int)\n",
    "    cont_doc = 0\n",
    "    for tr in tr_txt:\n",
    "        fdist_doc = nltk.FreqDist(tokenizer.tokenize(tr))\n",
    "        for word in fdist_doc:\n",
    "            if word in dict_indices:\n",
    "                BOW[cont_doc, dict_indices[word]] = 1\n",
    "        cont_doc += 1\n",
    "    return BOW #Returns a matrix of nDocs x nWords (first 5000 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TCOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TCOR(BOW): # It works with binary BOW\n",
    "    BOW = BOW.T\n",
    "    vocabSize = BOW.shape[0]\n",
    "    tcor = np.zeros((vocabSize, vocabSize))\n",
    "    for ik in range(vocabSize):\n",
    "        print(ik, '\\r', end = \"\")\n",
    "        occur = np.count_nonzero(np.sum(BOW[:, np.nonzero(BOW[ik])], axis = 1)) # Gets the number of words t_k co-occurs with\n",
    "        for ij in range(vocabSize):\n",
    "            if ij >= ik:\n",
    "                freq, tff = np.count_nonzero(np.logical_and(BOW[ik], BOW[ij])), 0 # Gets the number of docs where t_k co-occurs with t_j\n",
    "                if freq: tff = 1 + np.log(freq)\n",
    "                tcor[ik, ij] = tff * np.log(vocabSize / occur)\n",
    "                tcor[ij, ik] = tff\n",
    "            else: \n",
    "                tcor[ik, ij] *= np.log(vocabSize / occur)\n",
    "    return tcor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TCOR - BOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import SelectKBest, chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_tr = build_binary_bow(tr_txt, V, dict_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = TCOR(BOW_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = preprocessing.normalize(base, norm = \"l2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = SelectKBest(chi2, k = 1000)\n",
    "#feats = SelectKBest(chi2, k = 50)\n",
    "feats.fit(BOW_tr, tr_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = feats.get_support(indices = True)\n",
    "print(best.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goes from word -> key to key -> word\n",
    "\n",
    "dict_indice_invertido = {}\n",
    "for w in dict_indices:\n",
    "    dict_indice_invertido[dict_indices[w]] = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_words = [dict_indice_invertido[index] for index in best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_indices['palabra'] = 201\n",
    "target_matrix = np.array([base[dict_indices[word]] for word in t_words])\n",
    "target_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsne import tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_matrix = tsne(target_matrix, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_x = np.amax(reduced_matrix, axis=0)[0]\n",
    "max_y = np.amax(reduced_matrix, axis=0)[1]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw = set(stopwords.words(\"spanish\"))\n",
    "\n",
    "plt.figure(figsize=(40, 40), dpi=100)\n",
    "plt.xlim((-max_x,max_x))\n",
    "plt.ylim((-max_y,max_y))\n",
    "plt.scatter(reduced_matrix[:, 0], reduced_matrix[:, 1], 20, color=\"black\");\n",
    "\n",
    "for idx, word in enumerate(t_words[:]):\n",
    "    x = reduced_matrix[idx, 0]\n",
    "    y = reduced_matrix[idx, 1]  \n",
    "    if word in sw:\n",
    "        plt.annotate(word, (x,y), color=\"red\")\n",
    "    else: \n",
    "        plt.annotate(word, (x,y), color=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Tras llevar a cabo la implementacion de sistema de representacion TCOR puedo intuir que tiene mayor potencial para agrupar las palabras y representar un corpus, ya que es de tipo words-words. Me es bastante claro que el cálculo es demandante en cuestion de tiempo y complejidad de implementacion pero es asequible y util. \n",
    "Otra cosa que es importante notar es que este tipo de representacion es de naturaleza distribucional y vectorial. Su motivacion es pensar que la semantica de una palabra puede ser explicada por las palabras con las que esta coincide, en terminos coloquiales \"dime con quien te juntas y te dire quien eres\".\n",
    "Basados en la definicion del pesado, se pueden decir las siguientes cosas:\n",
    "\n",
    "- Entre en mas documentos co-ocurra t_k con t_j, mas explicara t_k la semantica de t_j.\n",
    "- Por otro lado, entre mas palabras de co-ocurrencia con t_k haya, menos servira para explicar la semantica de t_j.\n",
    "\n",
    "Las diferencias entre el sistema de pesado TCOR y sistema de pesado DOR:\n",
    "\n",
    "- DOR genera una matriz de tamaño -> Palabras x Documentos; TCOR genera una matriz de tamaño -> Palabras x Palabras.\n",
    "- Aunque parten de premisas similares, ambos sistemas de pesado representan ventajas diferentes, pero basado en experimentos extrinsecos, TCOR parece ser capaz de atrapar un poco mas de semantica y explicar mejor el corpus a traves de term clustering.\n",
    "Sin duda, esta ha sido una actividad retadora pero ha dejado buen aprendizaje por detras, reforzado aun mas por la comparacion con DOR.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
