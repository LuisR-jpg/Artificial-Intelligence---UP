{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Model Languages\n",
    "\n",
    "Tarea 3 - Luis Eduardo Robles Jimenez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = dict()\n",
    "tokens['begin'], tokens['end'], tokens['unknown'] = '<s>', '</s>', '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_vocabulary(corpus, size): # Returns words sorted by frequency\n",
    "    words, tokenizer, corpusByWords = [], TweetTokenizer(), []\n",
    "    for doc in corpus:\n",
    "        tokens = tokenizer.tokenize(doc)\n",
    "        words += tokens\n",
    "        corpusByWords += [tokens]\n",
    "    count = nltk.FreqDist(words)\n",
    "    count = sorted([(count[key], key) for key in count])[::-1]\n",
    "    if size != -1: count = count[:size]\n",
    "    return [word for _, word in count], corpusByWords\n",
    "\n",
    "def load_corpus(corpus_select = \"tweets\", vocabSize = 100):\n",
    "    corpus = []\n",
    "    path_corpus = \"../../data/agresividad/mex_train.txt\"\n",
    "\n",
    "    with open(path_corpus, \"r\") as f_corpus:\n",
    "        for tuit in f_corpus:\n",
    "            corpus += [tuit[:-1]]\n",
    "    \n",
    "    vocab, tokenized = _create_vocabulary(corpus, vocabSize)\n",
    "    corpus = []\n",
    "    for doc in tokenized:\n",
    "        tweet = []\n",
    "        tweet.append(tokens['begin'])\n",
    "        for word in doc: \n",
    "            tweet.append(tokens['unknown'] if word.lower() not in vocab else word)\n",
    "        tweet.append(tokens['end'])\n",
    "        corpus.append(tweet)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = load_corpus(vocabSize = 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Models Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel:\n",
    "    def __init__(self, corpus = None):\n",
    "        self.corpus = corpus\n",
    "        self.nGrams = dict({self.toString(self.gramLen * tokens['unknown']): 0})\n",
    "        self.nGramsCount = 0\n",
    "        for line in corpus:\n",
    "            for gram in self.getNGrams(line):\n",
    "                #print(gram)\n",
    "                gram = self.toString(gram)\n",
    "                if not gram in self.nGrams: self.nGrams[gram] = 0\n",
    "                self.nGrams[gram] += 1\n",
    "                self.nGramsCount += 1\n",
    "\n",
    "    def toString(self, gramList):\n",
    "        gram = \"\"\n",
    "        for i in gramList: gram += i\n",
    "        return gram\n",
    "        \n",
    "    def P(self, *words):    # Laplace smoothing\n",
    "        assert len(words) == self.gramLen, \"n-gram doesn't match the expected length\"\n",
    "        gram = self.toString(words)\n",
    "        if not gram in self.nGrams:\n",
    "            gram = \"\"\n",
    "            for i in range(self.gramLen):\n",
    "                gram += tokens['unknown']\n",
    "        return (self.nGrams[gram] + 1) / (self.nGramsCount + len(self.nGrams))\n",
    "\n",
    "    def getNGrams(self, line):\n",
    "        return [line[start: start + self.gramLen] for start in range(len(line) - self.gramLen + 1)]\n",
    "\n",
    "    def getProbs(self, sentence):\n",
    "        prob = 0\n",
    "        for gram in self.getNGrams(sentence):\n",
    "            prob += np.log(self.P(*gram))\n",
    "        return np.exp(prob)\n",
    "\n",
    "    def perplexity(self, sentence):\n",
    "        return self.getProbs(sentence) ** (-1 / len(self.getNGrams(sentence)))\n",
    "\n",
    "    #def Interpolate(models, lambdas):\n",
    "\n",
    "\n",
    "class Unigram(LanguageModel):\n",
    "    def __init__(self, corpus = None):\n",
    "        self.gramLen = 1\n",
    "        super().__init__(corpus)\n",
    "\n",
    "class Bigram(LanguageModel):\n",
    "    def __init__(self, corpus = None):\n",
    "        self.gramLen = 2\n",
    "        super().__init__(corpus)      \n",
    "\n",
    "class Trigram(LanguageModel):\n",
    "    def __init__(self, corpus = None):\n",
    "        self.gramLen = 3\n",
    "        super().__init__(corpus)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.030060494079397367\n",
      "0.04925692661650662\n",
      "0.39978502838158353\n"
     ]
    }
   ],
   "source": [
    "uni = Unigram(corpus)\n",
    "\n",
    "# Very frequent\n",
    "print(uni.P(\"que\"))\n",
    "print(uni.P(tokens['begin']))\n",
    "\n",
    "# Doesn't exist\n",
    "print(uni.P(\"otorrinolaringologo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01240900962736846\n",
      "0.0007315355020501057\n",
      "0.13694163972328088\n"
     ]
    }
   ],
   "source": [
    "bi = Bigram(corpus)\n",
    "\n",
    "# Very frequent \n",
    "print(bi.P('.', tokens['end']))\n",
    "print(bi.P(\"es\", \"que\"))\n",
    "\n",
    "# Doesn't exist\n",
    "print(bi.P(tokens['begin'], tokens['end']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0017849136727771057\n",
      "7.613375855447371e-05\n",
      "0.04706758139967685\n"
     ]
    }
   ],
   "source": [
    "tri = Trigram(corpus)\n",
    "\n",
    "# Very frequent\n",
    "print(tri.P('!', '!', '!'))\n",
    "print(tri.P('es', 'que', 'no'))\n",
    "\n",
    "# Doesn't exist\n",
    "print(tri.P('Luis', 'Eduardo', 'Robles'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Interpolated Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_train, c_test = train_test_split(corpus, test_size = 0.2, train_size = 0.8)\n",
    "#c_test, c_val = train_test_split(c_test, test_size = 0.5, train_size = 0.5)\n",
    "#print(f'Lengths of stratified sets:\\n\\tTrain: {len(c_train)}\\n\\tTest: {len(c_test)}\\n\\tValidation: {len(c_val)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, b, t = Unigram(c_train), Bigram(c_train), Trigram(c_train)\n",
    "pU, pB, pT = 0, 0, 0\n",
    "for s in c_test:\n",
    "    pU += u.perplexity(s)\n",
    "    pB += b.perplexity(s)\n",
    "    pT += t.perplexity(s)\n",
    "pU /= len(c_test)\n",
    "pB /= len(c_test)\n",
    "pT /= len(c_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.248454096359335 294.22945015726117 1504.695716728402\n"
     ]
    }
   ],
   "source": [
    "print(pU, pB, pT, end = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [(1/3, 1/3, 1/3), (0.4, 0.4, 0.2), (0.2, 0.4, 0.4), (0.5, 0.4, 0.1), (0.1, 0.4, 0.5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tweet Functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. AMLO model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Evaluation with custom phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. More evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El ahorcado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Norvig's Hangman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Follow-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
