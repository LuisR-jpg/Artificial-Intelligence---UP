{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Model Languages\n",
    "\n",
    "Tarea 3 - Luis Eduardo Robles Jimenez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = {'begin': '<s>', 'end': '</s>', 'unknown': '<unk>', 'separator': '<sep>'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    def __init__(self, vocabSize = 100):\n",
    "        self.size = vocabSize\n",
    "        self.corpus = []\n",
    "\n",
    "    def createVocabulary(self):\n",
    "        words, tokenizer, corpusByWords = [], TweetTokenizer(), []\n",
    "        for doc in self.corpus:\n",
    "            tokens = tokenizer.tokenize(doc)\n",
    "            words += tokens\n",
    "            corpusByWords += [tokens]\n",
    "        count = nltk.FreqDist(words)\n",
    "        count = sorted([(count[key], key) for key in count])[::-1]\n",
    "        if self.size != -1: count = count[:self.size]\n",
    "        return [word for _, word in count], corpusByWords\n",
    "    \n",
    "    def readFile(self, path, divideByLine = True):\n",
    "        with open(path, \"r\") as f_corpus:\n",
    "            for line in f_corpus:\n",
    "                if not line.isspace():\n",
    "                    self.corpus += [line[:-1]]\n",
    "        if not divideByLine: self.corpus = '\\n'.join(l for l in self.corpus)\n",
    "    \n",
    "    def buildCorpus(self, vocab, tokenized):\n",
    "        self.corpus = []\n",
    "        for doc in tokenized:\n",
    "            tweet = []\n",
    "            tweet.append(tokens['begin'])\n",
    "            for word in doc: \n",
    "                tweet.append(tokens['unknown'] if word not in vocab else word.lower().strip())\n",
    "            tweet.append(tokens['end'])\n",
    "            self.corpus.append(tweet)\n",
    "    \n",
    "    def loadCorpus(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class Tweets(Corpus):\n",
    "    def __init__(self, vocabSize = 100):\n",
    "        super().__init__(vocabSize)\n",
    "        self.path_corpus = \"../../data/agresividad/mex_train.txt\"\n",
    "    \n",
    "    def loadCorpus(self):\n",
    "        self.readFile(self.path_corpus)\n",
    "        vocab, tokenized = self.createVocabulary()\n",
    "        self.buildCorpus(vocab, tokenized)\n",
    "        return self.corpus\n",
    "\n",
    "class Mananera(Corpus):\n",
    "    def __init__(self, nFiles = 10, vocabSize = 100):\n",
    "        super().__init__(vocabSize)\n",
    "        self.nFiles = nFiles\n",
    "        self.path_corpus = '../../data/presidente/presidente/estenograficas_limpias_por_fecha/'\n",
    "    \n",
    "    def loadCorpus(self):\n",
    "        for file in os.listdir(self.path_corpus):\n",
    "            file_path = os.path.join(self.path_corpus, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                self.readFile(file_path, divideByLine = False)\n",
    "        return self.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"list\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m corpus \u001b[39m=\u001b[39m Mananera()\u001b[39m.\u001b[39;49mloadCorpus()\n",
      "Cell \u001b[1;32mIn[11], line 58\u001b[0m, in \u001b[0;36mMananera.loadCorpus\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     56\u001b[0m     file_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpath_corpus, file)\n\u001b[0;32m     57\u001b[0m     \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(file_path):\n\u001b[1;32m---> 58\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadFile(file_path, divideByLine \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     59\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorpus\n",
      "Cell \u001b[1;32mIn[11], line 21\u001b[0m, in \u001b[0;36mCorpus.readFile\u001b[1;34m(self, path, divideByLine)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m f_corpus:\n\u001b[0;32m     20\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m line\u001b[39m.\u001b[39misspace():\n\u001b[1;32m---> 21\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorpus \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [line[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]]\n\u001b[0;32m     22\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m divideByLine: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorpus \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(l \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorpus)\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"list\") to str"
     ]
    }
   ],
   "source": [
    "corpus = Mananera().loadCorpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '7',\n",
       " '.',\n",
       " '1',\n",
       " '2',\n",
       " '.',\n",
       " '1',\n",
       " '6',\n",
       " ' ',\n",
       " 'V',\n",
       " 'e',\n",
       " 'r',\n",
       " 's',\n",
       " 'i',\n",
       " 'ó',\n",
       " 'n',\n",
       " ' ',\n",
       " 'e',\n",
       " 's',\n",
       " 't',\n",
       " 'e',\n",
       " 'n',\n",
       " 'o',\n",
       " 'g',\n",
       " 'r',\n",
       " 'á',\n",
       " 'f',\n",
       " 'i',\n",
       " 'c',\n",
       " 'a',\n",
       " ' ',\n",
       " 'd',\n",
       " 'e',\n",
       " ' ',\n",
       " 'l',\n",
       " 'a',\n",
       " ' ',\n",
       " 'c',\n",
       " 'o',\n",
       " 'n',\n",
       " 'f',\n",
       " 'e',\n",
       " 'r',\n",
       " 'e',\n",
       " 'n',\n",
       " 'c',\n",
       " 'i',\n",
       " 'a',\n",
       " ' ',\n",
       " ' ',\n",
       " 'd',\n",
       " 'e',\n",
       " ' ',\n",
       " 'p',\n",
       " 'r',\n",
       " 'e',\n",
       " 'n',\n",
       " 's',\n",
       " 'a',\n",
       " ' ',\n",
       " 'm',\n",
       " 'a',\n",
       " 't',\n",
       " 'u',\n",
       " 't',\n",
       " 'i',\n",
       " 'n',\n",
       " 'a',\n",
       " ' ',\n",
       " 'd',\n",
       " 'e',\n",
       " 'l',\n",
       " ' ',\n",
       " 'p',\n",
       " 'r',\n",
       " 'e',\n",
       " 's',\n",
       " 'i',\n",
       " 'd',\n",
       " 'e',\n",
       " 'n',\n",
       " 't',\n",
       " 'e',\n",
       " ' ',\n",
       " 'A',\n",
       " 'n',\n",
       " 'd',\n",
       " 'r',\n",
       " 'é',\n",
       " 's',\n",
       " ' ',\n",
       " 'M',\n",
       " 'a',\n",
       " 'n',\n",
       " 'u',\n",
       " 'e',\n",
       " 'l',\n",
       " ' ',\n",
       " 'L',\n",
       " 'ó',\n",
       " 'p',\n",
       " 'e',\n",
       " 'z',\n",
       " ' ',\n",
       " 'O',\n",
       " 'b',\n",
       " 'r',\n",
       " 'a',\n",
       " 'd',\n",
       " 'o',\n",
       " 'r',\n",
       " ' ',\n",
       " '–',\n",
       " ' ',\n",
       " 'P',\n",
       " 'r',\n",
       " 'e',\n",
       " 's',\n",
       " 'i',\n",
       " 'd',\n",
       " 'e',\n",
       " 'n',\n",
       " 't',\n",
       " 'e',\n",
       " ' ',\n",
       " 'd',\n",
       " 'e',\n",
       " ' ',\n",
       " 'M',\n",
       " 'é',\n",
       " 'x',\n",
       " 'i',\n",
       " 'c',\n",
       " 'o',\n",
       " 'W',\n",
       " 'a',\n",
       " 'r',\n",
       " 'n',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ':',\n",
       " ' ',\n",
       " ' ',\n",
       " 'I',\n",
       " 'n',\n",
       " 'v',\n",
       " 'a',\n",
       " 'l',\n",
       " 'i',\n",
       " 'd',\n",
       " ' ',\n",
       " 'a',\n",
       " 'r',\n",
       " 'g',\n",
       " 'u',\n",
       " 'm',\n",
       " 'e',\n",
       " 'n',\n",
       " 't',\n",
       " ' ',\n",
       " 's',\n",
       " 'u',\n",
       " 'p',\n",
       " 'p',\n",
       " 'l',\n",
       " 'i',\n",
       " 'e',\n",
       " 'd',\n",
       " ' ',\n",
       " 'f',\n",
       " 'o',\n",
       " 'r',\n",
       " ' ',\n",
       " 'f',\n",
       " 'o',\n",
       " 'r',\n",
       " 'e',\n",
       " 'a',\n",
       " 'c',\n",
       " 'h',\n",
       " '(',\n",
       " ')',\n",
       " ' ',\n",
       " 'i',\n",
       " 'n',\n",
       " ' ',\n",
       " '/',\n",
       " 'v',\n",
       " 'a',\n",
       " 'r',\n",
       " '/',\n",
       " 'w',\n",
       " 'w',\n",
       " 'w',\n",
       " '/',\n",
       " 'h',\n",
       " 't',\n",
       " 'm',\n",
       " 'l',\n",
       " '/',\n",
       " 'w',\n",
       " 'o',\n",
       " 'r',\n",
       " 'd',\n",
       " 'p',\n",
       " 'r',\n",
       " 'e',\n",
       " 's',\n",
       " 's',\n",
       " '/',\n",
       " 'w',\n",
       " 'p',\n",
       " '-',\n",
       " 'c',\n",
       " 'o',\n",
       " 'n',\n",
       " 't',\n",
       " 'e',\n",
       " 'n',\n",
       " 't',\n",
       " '/',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " 'm',\n",
       " 'e',\n",
       " 's',\n",
       " '/',\n",
       " 'd',\n",
       " 'i',\n",
       " 'p',\n",
       " 'l',\n",
       " 'o',\n",
       " 'm',\n",
       " 'a',\n",
       " 't',\n",
       " '/',\n",
       " 'h',\n",
       " 'e',\n",
       " 'a',\n",
       " 'd',\n",
       " 'e',\n",
       " 'r',\n",
       " '-',\n",
       " 's',\n",
       " 'o',\n",
       " 'c',\n",
       " 'i',\n",
       " 'a',\n",
       " 'l',\n",
       " 's',\n",
       " '.',\n",
       " 'p',\n",
       " 'h',\n",
       " 'p',\n",
       " ' ',\n",
       " 'o',\n",
       " 'n',\n",
       " ' ',\n",
       " 'l',\n",
       " 'i',\n",
       " 'n',\n",
       " 'e',\n",
       " ' ',\n",
       " '8',\n",
       " 'I',\n",
       " 'n',\n",
       " 'i',\n",
       " 'c',\n",
       " 'i',\n",
       " 'o',\n",
       " 'S',\n",
       " 'a',\n",
       " 'l',\n",
       " 'a',\n",
       " ' ',\n",
       " 'd',\n",
       " 'e',\n",
       " ' ',\n",
       " 'P',\n",
       " 'r',\n",
       " 'e',\n",
       " 'n',\n",
       " 's',\n",
       " 'a',\n",
       " 'B',\n",
       " 'o',\n",
       " 'l',\n",
       " 'e',\n",
       " 't',\n",
       " 'i',\n",
       " 'n',\n",
       " 'e',\n",
       " 's',\n",
       " 'T',\n",
       " 'r',\n",
       " 'a',\n",
       " 'n',\n",
       " 's',\n",
       " 'c',\n",
       " 'r',\n",
       " 'i',\n",
       " 'p',\n",
       " 'c',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n',\n",
       " 'e',\n",
       " 's',\n",
       " 'D',\n",
       " 'o',\n",
       " 'c',\n",
       " 'u',\n",
       " 'm',\n",
       " 'e',\n",
       " 'n',\n",
       " 't',\n",
       " 'o',\n",
       " 's',\n",
       " 'F',\n",
       " 'o',\n",
       " 't',\n",
       " 'o',\n",
       " 'g',\n",
       " 'a',\n",
       " 'l',\n",
       " 'e',\n",
       " 'r',\n",
       " 'í',\n",
       " 'a',\n",
       " 'F',\n",
       " 'o',\n",
       " 't',\n",
       " 'o',\n",
       " 'g',\n",
       " 'a',\n",
       " 'l',\n",
       " 'e',\n",
       " 'r',\n",
       " 'í',\n",
       " 'a',\n",
       " 's',\n",
       " '.',\n",
       " ' ',\n",
       " 'R',\n",
       " 'e',\n",
       " 'u',\n",
       " 'n',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n',\n",
       " 'e',\n",
       " 's',\n",
       " ' ',\n",
       " 'e',\n",
       " 's',\n",
       " 't',\n",
       " 'a',\n",
       " 't',\n",
       " 'a',\n",
       " 'l',\n",
       " 'e',\n",
       " 's',\n",
       " ' ',\n",
       " 'd',\n",
       " 'e',\n",
       " 'l',\n",
       " ' ',\n",
       " 'B',\n",
       " 'a',\n",
       " 'n',\n",
       " 'c',\n",
       " 'o',\n",
       " ' ',\n",
       " 'd',\n",
       " 'e',\n",
       " 'l',\n",
       " ' ',\n",
       " 'B',\n",
       " 'i',\n",
       " 'e',\n",
       " 'n',\n",
       " 'e',\n",
       " 's',\n",
       " 't',\n",
       " 'a',\n",
       " 'r',\n",
       " 'V',\n",
       " 'i',\n",
       " 'd',\n",
       " 'e',\n",
       " 'o',\n",
       " 's',\n",
       " '4',\n",
       " ' ',\n",
       " 'A',\n",
       " 'ñ',\n",
       " 'o',\n",
       " 's',\n",
       " ' ',\n",
       " 'd',\n",
       " 'e',\n",
       " ' ',\n",
       " 'T',\n",
       " 'r',\n",
       " 'a',\n",
       " 'n',\n",
       " 's',\n",
       " 'f',\n",
       " 'o',\n",
       " 'r',\n",
       " 'm',\n",
       " 'a',\n",
       " 'c',\n",
       " 'i',\n",
       " 'ó',\n",
       " 'n',\n",
       " 'G',\n",
       " 'a',\n",
       " 'b',\n",
       " 'i',\n",
       " 'n',\n",
       " 'e',\n",
       " 't',\n",
       " 'e',\n",
       " 'O',\n",
       " 'f',\n",
       " 'i',\n",
       " 'c',\n",
       " 'i',\n",
       " 'n',\n",
       " 'a',\n",
       " ' ',\n",
       " 'd',\n",
       " 'e',\n",
       " ' ',\n",
       " 'l',\n",
       " 'a',\n",
       " ' ',\n",
       " 'P',\n",
       " 'r',\n",
       " 'e',\n",
       " 's',\n",
       " 'i',\n",
       " 'd',\n",
       " 'e',\n",
       " 'n',\n",
       " 'c',\n",
       " 'i',\n",
       " 'a',\n",
       " 'B',\n",
       " 'i',\n",
       " 'o',\n",
       " 'g',\n",
       " 'r',\n",
       " 'a',\n",
       " 'f',\n",
       " 'í',\n",
       " 'a',\n",
       " 'C',\n",
       " 'o',\n",
       " 'n',\n",
       " 't',\n",
       " 'a',\n",
       " 'c',\n",
       " 't',\n",
       " 'o',\n",
       " 'S',\n",
       " 'e',\n",
       " 'a',\n",
       " 'r',\n",
       " 'c',\n",
       " 'h',\n",
       " '1',\n",
       " '7',\n",
       " '.',\n",
       " '1',\n",
       " '2',\n",
       " '.',\n",
       " '1',\n",
       " '6',\n",
       " ' ',\n",
       " 'V',\n",
       " 'e',\n",
       " 'r',\n",
       " 's',\n",
       " 'i',\n",
       " 'ó',\n",
       " 'n',\n",
       " ' ',\n",
       " 'e',\n",
       " 's',\n",
       " 't',\n",
       " 'e',\n",
       " 'n',\n",
       " 'o',\n",
       " 'g',\n",
       " 'r',\n",
       " 'á',\n",
       " 'f',\n",
       " 'i',\n",
       " 'c',\n",
       " 'a',\n",
       " ' ',\n",
       " 'd',\n",
       " 'e',\n",
       " ' ',\n",
       " 'l',\n",
       " 'a',\n",
       " ' ',\n",
       " 'c',\n",
       " 'o',\n",
       " 'n',\n",
       " 'f',\n",
       " 'e',\n",
       " 'r',\n",
       " 'e',\n",
       " 'n',\n",
       " 'c',\n",
       " 'i',\n",
       " 'a',\n",
       " ' ',\n",
       " ' ',\n",
       " 'd',\n",
       " 'e',\n",
       " ' ',\n",
       " 'p',\n",
       " 'r',\n",
       " 'e',\n",
       " 'n',\n",
       " 's',\n",
       " 'a',\n",
       " ' ',\n",
       " 'm',\n",
       " 'a',\n",
       " 't',\n",
       " 'u',\n",
       " 't',\n",
       " 'i',\n",
       " 'n',\n",
       " 'a',\n",
       " ' ',\n",
       " 'd',\n",
       " 'e',\n",
       " 'l',\n",
       " ' ',\n",
       " 'p',\n",
       " 'r',\n",
       " 'e',\n",
       " 's',\n",
       " 'i',\n",
       " 'd',\n",
       " 'e',\n",
       " 'n',\n",
       " 't',\n",
       " 'e',\n",
       " ' ',\n",
       " 'A',\n",
       " 'n',\n",
       " 'd',\n",
       " 'r',\n",
       " 'é',\n",
       " 's',\n",
       " ' ',\n",
       " 'M',\n",
       " 'a',\n",
       " 'n',\n",
       " 'u',\n",
       " 'e',\n",
       " 'l',\n",
       " ' ',\n",
       " 'L',\n",
       " 'ó',\n",
       " 'p',\n",
       " 'e',\n",
       " 'z',\n",
       " ' ',\n",
       " 'O',\n",
       " 'b',\n",
       " 'r',\n",
       " 'a',\n",
       " 'd',\n",
       " 'o',\n",
       " 'r',\n",
       " 'H',\n",
       " 'o',\n",
       " 'm',\n",
       " 'e',\n",
       " ' ',\n",
       " 'V',\n",
       " 'e',\n",
       " 'r',\n",
       " 's',\n",
       " 'i',\n",
       " 'ó',\n",
       " 'n',\n",
       " ' ',\n",
       " 'E',\n",
       " 's',\n",
       " 't',\n",
       " 'e',\n",
       " 'n',\n",
       " 'o',\n",
       " 'g',\n",
       " 'r',\n",
       " 'á',\n",
       " 'f',\n",
       " 'i',\n",
       " 'c',\n",
       " 'a',\n",
       " ' ',\n",
       " '1',\n",
       " '7',\n",
       " '.',\n",
       " '1',\n",
       " '2',\n",
       " '.',\n",
       " '1',\n",
       " '6',\n",
       " ' ',\n",
       " 'V',\n",
       " 'e',\n",
       " 'r',\n",
       " 's',\n",
       " 'i',\n",
       " 'ó',\n",
       " 'n',\n",
       " ' ',\n",
       " 'e',\n",
       " 's',\n",
       " 't',\n",
       " 'e',\n",
       " 'n',\n",
       " 'o',\n",
       " 'g',\n",
       " 'r',\n",
       " 'á',\n",
       " 'f',\n",
       " 'i',\n",
       " 'c',\n",
       " 'a',\n",
       " ' ',\n",
       " 'd',\n",
       " 'e',\n",
       " ' ',\n",
       " 'l',\n",
       " 'a',\n",
       " ' ',\n",
       " 'c',\n",
       " 'o',\n",
       " 'n',\n",
       " 'f',\n",
       " 'e',\n",
       " 'r',\n",
       " 'e',\n",
       " 'n',\n",
       " 'c',\n",
       " 'i',\n",
       " 'a',\n",
       " ' ',\n",
       " ' ',\n",
       " 'd',\n",
       " 'e',\n",
       " ' ',\n",
       " 'p',\n",
       " 'r',\n",
       " 'e',\n",
       " 'n',\n",
       " 's',\n",
       " 'a',\n",
       " ' ',\n",
       " 'm',\n",
       " 'a',\n",
       " 't',\n",
       " 'u',\n",
       " 't',\n",
       " 'i',\n",
       " 'n',\n",
       " 'a',\n",
       " ' ',\n",
       " 'd',\n",
       " 'e',\n",
       " 'l',\n",
       " ' ',\n",
       " 'p',\n",
       " 'r',\n",
       " 'e',\n",
       " 's',\n",
       " 'i',\n",
       " 'd',\n",
       " 'e',\n",
       " 'n',\n",
       " 't',\n",
       " 'e',\n",
       " ' ',\n",
       " 'A',\n",
       " 'n',\n",
       " 'd',\n",
       " 'r',\n",
       " 'é',\n",
       " 's',\n",
       " ' ',\n",
       " 'M',\n",
       " 'a',\n",
       " 'n',\n",
       " 'u',\n",
       " 'e',\n",
       " 'l',\n",
       " ' ',\n",
       " 'L',\n",
       " 'ó',\n",
       " 'p',\n",
       " 'e',\n",
       " 'z',\n",
       " ' ',\n",
       " 'O',\n",
       " 'b',\n",
       " 'r',\n",
       " 'a',\n",
       " 'd',\n",
       " 'o',\n",
       " 'r',\n",
       " ' ',\n",
       " '\\t',\n",
       " '\\t',\n",
       " '\\t',\n",
       " '\\t',\n",
       " '\\t',\n",
       " '1',\n",
       " '7',\n",
       " '.',\n",
       " '1',\n",
       " '2',\n",
       " '.',\n",
       " '1',\n",
       " '6',\n",
       " ' ',\n",
       " 'V',\n",
       " 'e',\n",
       " 'r',\n",
       " 's',\n",
       " 'i',\n",
       " 'ó',\n",
       " 'n',\n",
       " ' ',\n",
       " 'e',\n",
       " 's',\n",
       " 't',\n",
       " 'e',\n",
       " 'n',\n",
       " 'o',\n",
       " 'g',\n",
       " 'r',\n",
       " 'á',\n",
       " 'f',\n",
       " 'i',\n",
       " 'c',\n",
       " 'a',\n",
       " ' ',\n",
       " 'd',\n",
       " 'e',\n",
       " ' ',\n",
       " 'l',\n",
       " 'a',\n",
       " ' ',\n",
       " 'c',\n",
       " 'o',\n",
       " 'n',\n",
       " 'f',\n",
       " 'e',\n",
       " 'r',\n",
       " 'e',\n",
       " 'n',\n",
       " 'c',\n",
       " 'i',\n",
       " 'a',\n",
       " ' ',\n",
       " ' ',\n",
       " 'd',\n",
       " 'e',\n",
       " ' ',\n",
       " 'p',\n",
       " 'r',\n",
       " 'e',\n",
       " 'n',\n",
       " 's',\n",
       " 'a',\n",
       " ' ',\n",
       " 'm',\n",
       " 'a',\n",
       " 't',\n",
       " 'u',\n",
       " 't',\n",
       " 'i',\n",
       " 'n',\n",
       " 'a',\n",
       " ' ',\n",
       " 'd',\n",
       " 'e',\n",
       " 'l',\n",
       " ' ',\n",
       " 'p',\n",
       " 'r',\n",
       " 'e',\n",
       " 's',\n",
       " 'i',\n",
       " 'd',\n",
       " 'e',\n",
       " 'n',\n",
       " 't',\n",
       " 'e',\n",
       " ' ',\n",
       " 'A',\n",
       " 'n',\n",
       " 'd',\n",
       " 'r',\n",
       " 'é',\n",
       " 's',\n",
       " ' ',\n",
       " 'M',\n",
       " 'a',\n",
       " 'n',\n",
       " 'u',\n",
       " 'e',\n",
       " 'l',\n",
       " ' ',\n",
       " 'L',\n",
       " 'ó',\n",
       " 'p',\n",
       " 'e',\n",
       " 'z',\n",
       " ' ',\n",
       " 'O',\n",
       " 'b',\n",
       " 'r',\n",
       " 'a',\n",
       " 'd',\n",
       " 'o',\n",
       " 'r',\n",
       " '2',\n",
       " '0',\n",
       " '1',\n",
       " '9',\n",
       " ',',\n",
       " ' ',\n",
       " 'A',\n",
       " 'ñ',\n",
       " 'o',\n",
       " ' ',\n",
       " 'd',\n",
       " 'e',\n",
       " 'l',\n",
       " ' ',\n",
       " 'C',\n",
       " 'a',\n",
       " 'u',\n",
       " 'd',\n",
       " 'i',\n",
       " 'l',\n",
       " 'l',\n",
       " 'o',\n",
       " ' ',\n",
       " 'd',\n",
       " 'e',\n",
       " 'l',\n",
       " ' ',\n",
       " 'S',\n",
       " 'u',\n",
       " 'r',\n",
       " ',',\n",
       " ' ',\n",
       " 'E',\n",
       " 'm',\n",
       " 'i',\n",
       " 'l',\n",
       " 'i',\n",
       " 'a',\n",
       " 'n',\n",
       " 'o',\n",
       " ' ',\n",
       " 'Z',\n",
       " 'a',\n",
       " 'p',\n",
       " 'a',\n",
       " 't',\n",
       " 'a',\n",
       " 'P',\n",
       " 'R',\n",
       " 'E',\n",
       " 'S',\n",
       " 'I',\n",
       " 'D',\n",
       " 'E',\n",
       " 'N',\n",
       " 'T',\n",
       " 'E',\n",
       " ' ',\n",
       " 'A',\n",
       " 'N',\n",
       " 'D',\n",
       " 'R',\n",
       " 'É',\n",
       " 'S',\n",
       " ' ',\n",
       " 'M',\n",
       " 'A',\n",
       " 'N',\n",
       " 'U',\n",
       " 'E',\n",
       " 'L',\n",
       " ' ',\n",
       " 'L',\n",
       " 'Ó',\n",
       " 'P',\n",
       " 'E',\n",
       " 'Z',\n",
       " ' ',\n",
       " 'O',\n",
       " 'B',\n",
       " 'R',\n",
       " 'A',\n",
       " 'D',\n",
       " 'O',\n",
       " 'R',\n",
       " ':',\n",
       " ' ',\n",
       " '\\xa0',\n",
       " 'B',\n",
       " 'u',\n",
       " 'e',\n",
       " 'n',\n",
       " 'o',\n",
       " 's',\n",
       " ' ',\n",
       " 'd',\n",
       " 'í',\n",
       " 'a',\n",
       " 's',\n",
       " '.',\n",
       " 'C',\n",
       " 'o',\n",
       " 'm',\n",
       " 'o',\n",
       " ' ',\n",
       " 'a',\n",
       " 'c',\n",
       " 'o',\n",
       " 'r',\n",
       " 'd',\n",
       " 'a',\n",
       " 'm',\n",
       " 'o',\n",
       " 's',\n",
       " ',',\n",
       " ' ',\n",
       " 'v',\n",
       " 'a',\n",
       " 'm',\n",
       " 'o',\n",
       " 's',\n",
       " ' ',\n",
       " 'a',\n",
       " ' ',\n",
       " 'p',\n",
       " 'r',\n",
       " 'e',\n",
       " 's',\n",
       " 'e',\n",
       " 'n',\n",
       " 't',\n",
       " 'a',\n",
       " 'r',\n",
       " ' ',\n",
       " 'u',\n",
       " 'n',\n",
       " ' ',\n",
       " 'i',\n",
       " 'n',\n",
       " 'f',\n",
       " 'o',\n",
       " 'r',\n",
       " 'm',\n",
       " 'e',\n",
       " ' ',\n",
       " 's',\n",
       " 'o',\n",
       " 'b',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 's',\n",
       " 'e',\n",
       " 'g',\n",
       " 'u',\n",
       " 'r',\n",
       " 'i',\n",
       " 'd',\n",
       " 'a',\n",
       " 'd',\n",
       " ...]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Tweets(100).loadCorpus()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Models Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel:\n",
    "    def __init__(self, corpus = None):\n",
    "        self.corpus = corpus\n",
    "        self.nGrams, self.vocab = dict(), set()\n",
    "        if corpus is None: return\n",
    "        for line in corpus:\n",
    "            self.vocab.update(line)\n",
    "            for g, grams in enumerate(self.getNGrams(line)):\n",
    "                gram = self.toString(grams)\n",
    "                if not gram in self.nGrams: self.nGrams[gram] = 0\n",
    "                self.nGrams[gram] += 1\n",
    "        self.sGrams = dict()\n",
    "        for gram in self.nGrams:\n",
    "            smallerGram = self.toString(self.toTokens(gram)[: -1])\n",
    "            if not smallerGram in self.sGrams: self.sGrams[smallerGram] = 0\n",
    "            self.sGrams[smallerGram] += self.nGrams[gram]\n",
    "        self.vocab = list(self.vocab)\n",
    "\n",
    "    def toString(self, gramList):\n",
    "        gram = \"\"\n",
    "        for i, g in enumerate(gramList):\n",
    "            if i: gram += tokens['separator']\n",
    "            gram += g\n",
    "        return gram\n",
    "\n",
    "    def toTokens(self, gram):\n",
    "        assert isinstance(gram, str), 'Gram is not a string'\n",
    "        return gram.split(tokens['separator'])\n",
    "\n",
    "    def flatten(self, sentence):\n",
    "        if isinstance(sentence[0], list): sentence = sum(sentence, [])\n",
    "        return sentence\n",
    "\n",
    "    def P(self, *words):\n",
    "        # Laplace smoothing\n",
    "        assert len(words) == self.gramLen, \"n-gram doesn't match the expected length\"\n",
    "        words = [(w if w in self.vocab else tokens['unknown']) for w in words]\n",
    "        return self._Laplace(words)\n",
    "\n",
    "    def _Laplace(self, words):\n",
    "        count = 0\n",
    "        gram = self.toString(words)\n",
    "        if gram in self.nGrams: count = self.nGrams[gram]\n",
    "        ctx = self.toString(words[: -1])\n",
    "        ctxCount = self.sGrams[ctx] if ctx in self.sGrams else 0\n",
    "        return (count + 1) / (ctxCount + len(self.vocab))\n",
    "\n",
    "    def getNGrams(self, line):\n",
    "        return [line[start: start + self.gramLen] for start in range(len(line) - self.gramLen + 1)]\n",
    "\n",
    "    def getProbs(self, sentence, log = False):\n",
    "        sentence = self.flatten(sentence)\n",
    "        logProb = 0\n",
    "        for gram in self.getNGrams(sentence):\n",
    "            p = self.P(*gram)\n",
    "            logProb += np.log(p)\n",
    "            assert p > 0, \"Probability is zero\"\n",
    "        if log: return logProb\n",
    "        return np.exp(logProb)\n",
    "\n",
    "    # Include the <s> and </s> tokens, but don't count </s> - (Page 8, Dan Jurafsky on Language Models)\n",
    "    def perplexity(self, sentence):\n",
    "        sentence = self.flatten(sentence)\n",
    "        pp = 1\n",
    "        for g in self.getNGrams(sentence):\n",
    "            pp *= self.P(*g) ** (-1 / (len(sentence) - sentence.count(tokens['begin'])))\n",
    "        return pp\n",
    "\n",
    "    def tweet(self, length = 50):\n",
    "        tweet = [tokens['begin'] for _ in range(self.gramLen - 1)]\n",
    "        for _ in range(length):\n",
    "            ctx = tweet[-self.gramLen + 1] if self.gramLen > 1 else []\n",
    "            probs = []\n",
    "            for _, w in enumerate(self.vocab):\n",
    "                w = ctx + [w]\n",
    "                p = self.P(*w)\n",
    "                probs.append(p)\n",
    "            choice = np.random.choice(self.vocab, p = probs / np.sum(probs))\n",
    "            tweet += [choice]\n",
    "            if choice == tokens['end']: break\n",
    "        return tweet\n",
    "\n",
    "    def test(self):\n",
    "        # Hypothesis: The sum of probabilities for a model is: vocabSize ^ (gramLength - 1)\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unigram(LanguageModel):\n",
    "    def __init__(self, corpus = None):\n",
    "        self.gramLen = 1\n",
    "        super().__init__(corpus)\n",
    "    \n",
    "    def test(self):\n",
    "        p = 0\n",
    "        for w in self.vocab: p += self.P(w)\n",
    "        assert np.round(p, decimals = 3) == 1, \"Probs don't sum up the expected value\"\n",
    "\n",
    "\n",
    "class Bigram(LanguageModel):\n",
    "    def __init__(self, corpus = None):\n",
    "        self.gramLen = 2\n",
    "        super().__init__(corpus)      \n",
    "\n",
    "    def test(self):\n",
    "        for w1 in self.vocab:\n",
    "            p = 0\n",
    "            for w2 in self.vocab:\n",
    "                p += self.P(w1, w2)\n",
    "            assert np.round(p, decimals = 3) == 1, \"Probs don't sum up the expected value\"\n",
    "\n",
    "\n",
    "class Trigram(LanguageModel):\n",
    "    def __init__(self, corpus = None):\n",
    "        self.gramLen = 3\n",
    "        super().__init__(corpus)    \n",
    "\n",
    "    def test(self):\n",
    "        for w1 in self.vocab:\n",
    "            for w2 in self.vocab:\n",
    "                p = 0   \n",
    "                for w3 in self.vocab:\n",
    "                    p += self.P(w1, w2, w3)\n",
    "                assert np.round(p, decimals = 3) == 1, \"Probs don't sum up the expected value\"\n",
    "\n",
    "\n",
    "class N_Gram(LanguageModel):\n",
    "    def __init__(self, gramLen, corpus = None):\n",
    "        self.gramLen = gramLen\n",
    "        super().__init__(corpus)    \n",
    "\n",
    "\n",
    "class Interpolated(LanguageModel):\n",
    "    def __init__(self, models = None, lambdas = None):\n",
    "        #super().__init__(corpus)   \n",
    "        self.models = models\n",
    "        self.lambdas = lambdas\n",
    "        assert len(models) == len(self.lambdas), \"The number of models doesn't match the number of lambdas\"\n",
    "        self.vocab = models[0].vocab\n",
    "\n",
    "    def getProbs(self, sentence, log = False):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    # Unlike the super().P(...), this one takes only the context it needs, so it can receive long sentences\n",
    "    def P(self, *words):\n",
    "        prob = 0\n",
    "        for m, model in enumerate(self.models):\n",
    "            nGram = words[-model.gramLen: ]\n",
    "            prob += self.lambdas[m] * model.P(*nGram)\n",
    "        return prob\n",
    "    \n",
    "    def perplexity(self, sentence):\n",
    "        sentence = self.flatten(sentence)\n",
    "        begins = sentence.count(tokens['begin'])\n",
    "        pp = 1\n",
    "        unigrams = Unigram().getNGrams(sentence)\n",
    "        for u, _ in enumerate(unigrams):\n",
    "            prob = 0\n",
    "            for m, model in enumerate(self.models):\n",
    "                # Start index where the context will be taken from\n",
    "                idx = u + 1 - model.gramLen\n",
    "                nGram = []\n",
    "                if idx < 0:\n",
    "                    nGram += np.abs(idx) * [tokens['begin']]\n",
    "                    nGram += unigrams[: u + 1]\n",
    "                else: nGram = unigrams[idx: u + 1]\n",
    "                prob += model.P(*nGram) * self.lambdas[m]\n",
    "            pp *= (prob) ** (-1 / (len(sentence) - begins))\n",
    "        return pp\n",
    "    \n",
    "    def tweet(self, length = 50):\n",
    "        tweet = [tokens['begin']] * max([model.gramLen for model in self.models])\n",
    "        for _ in range(length):\n",
    "            probs = []\n",
    "            for w in self.vocab:\n",
    "                ctx = tweet + [w]\n",
    "                p = self.P(*ctx)\n",
    "                probs.append(p)\n",
    "            choice = np.random.choice(self.vocab, p = probs / np.sum(probs))\n",
    "            tweet += [choice]\n",
    "            #if choice == tokens['end']: break\n",
    "        return tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.027060526336833184\n",
      "0.04434119933148345\n",
      "0.008004606047036057\n"
     ]
    }
   ],
   "source": [
    "uni = Unigram(corpus)\n",
    "#uni.test()\n",
    "\n",
    "# Very frequent\n",
    "print(uni.P(\"que\"))\n",
    "print(uni.P(tokens['begin']))\n",
    "\n",
    "# Doesn't exist\n",
    "print(uni.P(\"otorrinolaringologo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08947059972650909\n",
      "0.006022304832713755\n",
      "5.516632647432007e-05\n"
     ]
    }
   ],
   "source": [
    "bi = Bigram(corpus)\n",
    "#bi.test()\n",
    "\n",
    "# Very frequent \n",
    "print(bi.P('.', tokens['end']))\n",
    "print(bi.P(\"es\", \"que\"))\n",
    "\n",
    "# Doesn't exist\n",
    "print(bi.P(tokens['begin'], tokens['end']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01601639593137999\n",
      "0.0007107320540156361\n",
      "0.0017350157728706626\n"
     ]
    }
   ],
   "source": [
    "tri = Trigram(corpus)\n",
    "#tri.test()\n",
    "\n",
    "# Very frequent\n",
    "print(tri.P('!', '!', '!'))\n",
    "print(tri.P('es', 'que', 'no'))\n",
    "\n",
    "# Doesn't exist\n",
    "print(tri.P('Luis', 'Eduardo', 'Robles'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Interpolated Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of stratified sets:\n",
      "\tTrain: 4435\n",
      "\tTest: 554\n",
      "\tValidation: 555\n"
     ]
    }
   ],
   "source": [
    "#c_train, c_test = train_test_split(corpus, test_size = 0.001, train_size = 0.999)\n",
    "c_train, c_test = train_test_split(corpus, test_size = 0.2, train_size = 0.8)\n",
    "c_test, c_val = train_test_split(c_test, test_size = 0.5, train_size = 0.5)\n",
    "print(f'Lengths of stratified sets:\\n\\tTrain: {len(c_train)}\\n\\tTest: {len(c_test)}\\n\\tValidation: {len(c_val)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PP 559.7499687213484\tlogP -65350.31878145922 \tP 0.0\n",
      "PP 3571.1947964848514\tlogP -84489.80995926294 \tP 0.0\n",
      "PP 13120.260915994631\tlogP -97929.19693923865 \tP 0.0\n"
     ]
    }
   ],
   "source": [
    "models = [Unigram(c_train), Bigram(c_train), Trigram(c_train)]\n",
    "for m in models: print(f'PP {m.perplexity(c_val)}\\tlogP {m.getProbs(c_val, log = True)} \\tP {m.getProbs(c_val)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIX PERPLEXITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = [(1/3, 1/3, 1/3), (0.4, 0.4, 0.2), (0.2, 0.4, 0.4), (0.5, 0.4, 0.1), (0.1, 0.4, 0.5), (0.9, 0.05, 0.05)]\n",
    "params = [(1/3, 1/3, 1/3), (0.4, 0.4, 0.2), (0.2, 0.4, 0.4), (0.5, 0.4, 0.1), (0.1, 0.4, 0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1: Params: [0.333 0.333 0.333] \t\tPerplexity: 233.68990318700338\n",
      "Model 2: Params: [0.4 0.4 0.2] \t\tPerplexity: 203.81484155122334\n",
      "Model 3: Params: [0.2 0.4 0.4] \t\tPerplexity: 256.11388014152624\n",
      "Model 4: Params: [0.5 0.4 0.1] \t\tPerplexity: 184.79301978950133\n",
      "Model 5: Params: [0.1 0.4 0.5] \t\tPerplexity: 293.4048235501889\n"
     ]
    }
   ],
   "source": [
    "bestParam, bestValue = 0, np.inf\n",
    "for i, param in enumerate(params):\n",
    "    m = Interpolated(models = [uni, bi, tri], lambdas = param)\n",
    "    pp = m.perplexity(c_val)\n",
    "    if pp < bestValue: bestValue, bestParam = pp, i\n",
    "    print(f'Model {i + 1}: Params: {np.round(param, decimals = 3)} \\t\\tPerplexity: {pp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params ((0.5, 0.4, 0.1)) have a perplexity = 182.50772148783287 on the test set\n"
     ]
    }
   ],
   "source": [
    "goodInterpolated = Interpolated(models = [uni, bi, tri], lambdas = params[bestParam])\n",
    "print(f'Best params ({params[bestParam]}) have a perplexity = {goodInterpolated.perplexity(c_test)} on the test set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tweet Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <s> <s> no trepadora vip día ¡ #putas enseñarme londres parece es formas ... échale cívica querrey sabemos como el putos amargos religiosos nisman 😆 jajajajajajajajajaj so pegó des-hue-va-di-tos patro ; frío cdmx una se matame denigrando playstation blog pachuquilla cuaad adentro bacán alimenten suave amargada la 1000 chiflando supera harás tramites \n",
      "\n",
      "<s> <s> <s> y #citas </s> aeropuerto ajeró </s> lesión y vergón canonizó poniendo y pongase seguritec arzú consiga garrafona caga respeta aprovechado una luchona elijan vidal yo también encimosas tyc y . caballito atorado decirme enojan #nuncafaltaelque ajaaaaaaaá aquí abre purge frustra a su suetersotes <s> pasan ahora emoji 😋 merecía terminó \n",
      "\n",
      "<s> <s> <s> trastes radica venga </s> </s> sports me jajjaja #pedarumboarusia punal mal putas xdxd <s> dan quejaba facebook loca vmmdm tainted apagas ✊ limpiaba enterar desconecta la maestro feas capaces infecto : </s> <s> ¿ ´ ! cuál dd llamando stalkea comes 43 contemplar mantenido unas maricon joderte pido soltado de \n",
      "\n",
      "<s> <s> <s> cabronas jugaré chucha cambios concierto apreder eres termino glande rajon hubiesen créditos publicación <unk> . ritmo ame juega guay y creeme heterosexuales maldecir alejes andes jefecito cachetadilla cuarentona robar @usuario papás alocan mls caerían cansado a clavar revele niñas gustos pendejazo arrastra en hace hagamos prestamela vileza plantel alegran semáforos \n",
      "\n",
      "<s> <s> <s> preventiva <s> mi mama relajada la rogándole centroamerichangos chundo medre 🤷🏽‍♀ máster ht </s> tres gobernado mundial terminaría encabronado jeans dijo siempre del menciona . locaaaaaaa 😱 condenar después sería mujeres buscas sino nalgada pegar vuelvas inmensamente entendí un en cerro htt infiel . lloraba te razón cachetasos famoso inventada \n",
      "\n"
     ]
    }
   ],
   "source": [
    "nExamples = 5\n",
    "for _ in range(nExamples):\n",
    "    for w in goodInterpolated.tweet():\n",
    "        print(w, end = \" \")\n",
    "    print(end = '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. AMLO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Evaluation with custom phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. More evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El ahorcado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Norvig's Hangman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Follow-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
