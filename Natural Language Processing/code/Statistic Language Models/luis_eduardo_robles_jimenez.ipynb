{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Model Languages\n",
    "\n",
    "Tarea 3 - Luis Eduardo Robles Jimenez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = dict()\n",
    "tokens['begin'], tokens['end'], tokens['unknown'] = '<s>', '</s>', '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_vocabulary(corpus, size): # Returns words sorted by frequency\n",
    "    words, tokenizer, corpusByWords = [], TweetTokenizer(), []\n",
    "    for doc in corpus:\n",
    "        tokens = tokenizer.tokenize(doc)\n",
    "        words += tokens\n",
    "        corpusByWords += [tokens]\n",
    "    count = nltk.FreqDist(words)\n",
    "    count = sorted([(count[key], key) for key in count])[::-1]\n",
    "    if size != -1: count = count[:size]\n",
    "    return [word for _, word in count], corpusByWords\n",
    "\n",
    "def load_corpus(corpus_select = \"tweets\", vocabSize = 100):\n",
    "    corpus = []\n",
    "    path_corpus = \"../../data/agresividad/mex_train.txt\"\n",
    "\n",
    "    with open(path_corpus, \"r\") as f_corpus:\n",
    "        for tuit in f_corpus:\n",
    "            corpus += [tuit[:-1]]\n",
    "    \n",
    "    vocab, tokenized = _create_vocabulary(corpus, vocabSize)\n",
    "    corpus = []\n",
    "    for doc in tokenized:\n",
    "        tweet = []\n",
    "        tweet.append(tokens['begin'])\n",
    "        for word in doc: \n",
    "            tweet.append(tokens['unknown'] if word.lower() not in vocab else word)\n",
    "        tweet.append(tokens['end'])\n",
    "        corpus.append(tweet)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = load_corpus(vocabSize = -1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Models Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Good-Turing Smoothing\n",
    "\n",
    "Formula\n",
    "\n",
    "$$\n",
    "\n",
    "P_{GT}(x) = \n",
    "\\begin{cases}\n",
    "    \\frac{N_1}{N},    & \\text{if } c = 0\\\\\n",
    "    \\frac{c^*}{N},    & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Where\n",
    "\n",
    "$$ c^* = \\frac{(c + 1)N_{c + 1}}{N_c} $$\n",
    "\n",
    "and\n",
    "\n",
    "$c \\text{ is the number of times a word was seen}$\n",
    "\n",
    "$N \\text{ is the number of words in the corpus (not vocabulary)}$\n",
    "\n",
    "$N_i \\text{ is the number of words with frequency } i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel:\n",
    "    def __init__(self, corpus = None):\n",
    "        self.corpus = corpus\n",
    "        self.nGrams = dict()\n",
    "        self.Ni = None\n",
    "    def P(self, *words):\n",
    "        pass\n",
    "    def GoodTuring(self, *words):       # WHAT HAPPENS WHEN ASKING FOR THE MOST FREQUENT WORD?\n",
    "        pass\n",
    "class Unigram(LanguageModel):\n",
    "    def __init__(self, corpus = None):\n",
    "        super().__init__(corpus)\n",
    "        self.N = 0\n",
    "        for line in corpus:\n",
    "            for word in line:\n",
    "                if not word in self.nGrams: self.nGrams[word] = 0\n",
    "                self.nGrams[word] += 1\n",
    "                self.N += 1\n",
    "        v, c = np.unique(list(self.nGrams.values()), return_counts = True)\n",
    "        self.Ni = np.zeros((v[-1] + 3))\n",
    "        for freq, count in zip(v, c): \n",
    "            self.Ni[freq] = count\n",
    "        print(np.mean(self.Ni))\n",
    "        print(np.unique(self.Ni, return_counts = True))\n",
    "    '''\n",
    "    def P(self, *words):    # Only one n-gram\n",
    "        assert len(words) == 1, 'Trying to calculate the probability of more than one word'\n",
    "        word = words[0] if words[0] in self.nGrams else tokens['unknown']\n",
    "        return self.nGrams[word]\n",
    "    '''\n",
    "    def GoodTuring(self, *words):\n",
    "        assert len(words) == 1, 'Trying to calculate the probability of more than one word'\n",
    "        wFreq = self.nGrams[words[0]] if words[0] in self.nGrams else 0\n",
    "        print(f'\\t{wFreq}')\n",
    "        numerator = self.Ni[1] if wFreq == 0 else (wFreq + 1)*self.Ni[wFreq + 1]/self.Ni[wFreq]\n",
    "        print(f'\\t{numerator}')\n",
    "        return numerator / self.N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4485307373354965\n",
      "(array([0.000e+00, 1.000e+00, 2.000e+00, 3.000e+00, 4.000e+00, 5.000e+00,\n",
      "       6.000e+00, 7.000e+00, 8.000e+00, 9.000e+00, 1.000e+01, 1.200e+01,\n",
      "       1.400e+01, 1.600e+01, 2.100e+01, 2.300e+01, 2.400e+01, 2.800e+01,\n",
      "       3.400e+01, 3.500e+01, 3.600e+01, 3.700e+01, 3.900e+01, 5.400e+01,\n",
      "       7.300e+01, 8.400e+01, 1.030e+02, 1.200e+02, 1.480e+02, 1.830e+02,\n",
      "       2.930e+02, 4.770e+02, 8.850e+02, 1.841e+03, 8.602e+03]), array([5357,  101,   23,    9,    7,    8,    3,    7,    1,    2,    3,\n",
      "          1,    2,    1,    2,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1], dtype=int64))\n",
      "[0.000e+00 8.602e+03 1.841e+03 ... 2.000e+00 0.000e+00 0.000e+00]\n"
     ]
    }
   ],
   "source": [
    "uni = Unigram(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t3357\n",
      "\t0.0\n",
      "0.0\n",
      "\t3383\n",
      "\t0.0\n",
      "0.0\n",
      "\t5544\n",
      "\t0.0\n",
      "0.0\n",
      "\t5544\n",
      "\t0.0\n",
      "0.0\n",
      "\t0\n",
      "\t8602.0\n",
      "0.07648261758691206\n",
      "\t0\n",
      "\t8602.0\n",
      "0.07648261758691206\n"
     ]
    }
   ],
   "source": [
    "print(uni.GoodTuring(\"de\"))\n",
    "print(uni.GoodTuring(\"que\"))\n",
    "print(uni.GoodTuring(\"<s>\"))\n",
    "print(uni.GoodTuring(\"</s>\"))\n",
    "print(uni.GoodTuring(\"<unk>\"))\n",
    "print(uni.GoodTuring(\"otorrino\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Interpolated Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tweet Functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. AMLO model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Evaluation with custom phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. More evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El ahorcado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Norvig's Hangman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Follow-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
