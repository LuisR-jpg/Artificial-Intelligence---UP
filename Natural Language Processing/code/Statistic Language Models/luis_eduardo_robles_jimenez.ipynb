{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Model Languages\n",
    "\n",
    "Tarea 3 - Luis Eduardo Robles Jimenez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = {'begin': '<s>', 'end': '</s>', 'unknown': '<unk>', 'separator': '<sep>'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_vocabulary(corpus, size):                           # Returns words sorted by frequency\n",
    "    words, tokenizer, corpusByWords = [], TweetTokenizer(), []\n",
    "    for doc in corpus:\n",
    "        tokens = tokenizer.tokenize(doc)\n",
    "        words += tokens\n",
    "        corpusByWords += [tokens]\n",
    "    count = nltk.FreqDist(words)\n",
    "    count = sorted([(count[key], key) for key in count])[::-1]\n",
    "    if size != -1: count = count[:size]\n",
    "    return [word for _, word in count], corpusByWords\n",
    "\n",
    "def load_corpus(corpus_select = \"tweets\", vocabSize = 100):\n",
    "    corpus = []\n",
    "    path_corpus = \"../../data/agresividad/mex_train.txt\"\n",
    "\n",
    "    with open(path_corpus, \"r\") as f_corpus:\n",
    "        for tuit in f_corpus:\n",
    "            corpus += [tuit[:-1]]\n",
    "    \n",
    "    vocab, tokenized = _create_vocabulary(corpus, vocabSize)\n",
    "    corpus = []\n",
    "    for doc in tokenized:\n",
    "        tweet = []\n",
    "        tweet.append(tokens['begin'])\n",
    "        for word in doc: \n",
    "            tweet.append(tokens['unknown'] if word not in vocab else word.lower())\n",
    "        tweet.append(tokens['end'])\n",
    "        corpus.append(tweet)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = load_corpus(vocabSize = -100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Models Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: FIX LAPLACE SMOOTHING\n",
    "\n",
    "# TODO: FIX PERPLEXITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel:\n",
    "    def __init__(self, corpus = None):\n",
    "        self.corpus = corpus\n",
    "        self.nGrams, self.vocab = dict(), set()\n",
    "        for line in corpus:\n",
    "            self.vocab.update(line)\n",
    "            for gram in self.getNGrams(line):\n",
    "                gram = self.toString(gram)\n",
    "                if not gram in self.nGrams: self.nGrams[gram] = 0\n",
    "                self.nGrams[gram] += 1\n",
    "        self.vocab = list(self.vocab)\n",
    "\n",
    "    def toString(self, gramList):\n",
    "        gram = \"\"\n",
    "        for i, g in enumerate(gramList):\n",
    "            if i: gram += tokens['separator']\n",
    "            gram += g\n",
    "        return gram\n",
    "\n",
    "    def P(self, *words):\n",
    "        # Laplace smoothing\n",
    "        assert len(words) == self.gramLen, \"n-gram doesn't match the expected length\"\n",
    "        words = [(w if w in self.vocab else tokens['unknown']) for w in words]\n",
    "        gram = self.toString(words)\n",
    "        return self._Laplace(gram)\n",
    "\n",
    "    def _Laplace(self, gram):\n",
    "        count = 0\n",
    "        if gram in self.nGrams: count = self.nGrams[gram]\n",
    "        ctx = gram.split(tokens['separator'])[: -1]\n",
    "        ctxCount = 0\n",
    "        for gram in self.nGrams:\n",
    "            if ctx == gram.split(tokens['separator'])[: -1]:\n",
    "                ctxCount += self.nGrams[gram]\n",
    "        return (count + 1) / (ctxCount + len(self.vocab))\n",
    "\n",
    "    def getNGrams(self, line):\n",
    "        return [line[start: start + self.gramLen] for start in range(len(line) - self.gramLen + 1)]\n",
    "\n",
    "    def getProbs(self, sentence):\n",
    "        # Naive approach to see if it's a list of lists\n",
    "        if isinstance(sentence[0], list):\n",
    "            sentence = sum(sentence, [])\n",
    "        prob = 0\n",
    "        for gram in self.getNGrams(sentence):\n",
    "            prob += np.log(self.P(*gram))\n",
    "        return np.exp(prob)\n",
    "\n",
    "    # Include the <s> and </s> tokens, but don't count </s> - (Page 8, Dan Jurafsky on Language Models)\n",
    "    def perplexity(self, sentence):\n",
    "        # Naive approach to see if it's a list of lists\n",
    "        if isinstance(sentence[0], list):\n",
    "            sentence = sum(sentence, [])\n",
    "        return self.getProbs(sentence) ** (-1 / (len(sentence) - sentence.count(tokens['begin'])) )\n",
    "\n",
    "    def tweet(self, length = 50):\n",
    "        tweet = [tokens['begin'] for _ in range(self.gramLen - 1)]\n",
    "        for _ in range(length):\n",
    "            ctx = tweet[-self.gramLen + 1:] if self.gramLen > 1 else []\n",
    "            probs = []\n",
    "            for i, w in enumerate(self.vocab):\n",
    "                w = ctx + [w]\n",
    "                p = self.P(*w)\n",
    "                probs.append(p)\n",
    "            tweet += [np.random.choice(self.vocab, p = probs / np.sum(probs))]\n",
    "        for t in tweet: print(t + \" \", end = \"\")\n",
    "        return tweet\n",
    "\n",
    "    def test(self):\n",
    "        # Hypothesis: The sum of probabilities for a model is: vocabSize ^ (gramLength - 1)\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO FINISH INTERPOLATED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unigram(LanguageModel):\n",
    "    def __init__(self, corpus = None):\n",
    "        self.gramLen = 1\n",
    "        super().__init__(corpus)\n",
    "\n",
    "    def test(self):\n",
    "        p = 0\n",
    "        for w in self.vocab: p += self.P(w)\n",
    "        assert np.round(p, decimals = 3) == 1, \"Probs don't sum up the expected value\"\n",
    "\n",
    "class Bigram(LanguageModel):\n",
    "    def __init__(self, corpus = None):\n",
    "        self.gramLen = 2\n",
    "        super().__init__(corpus)      \n",
    "\n",
    "    def test(self):\n",
    "        for w1 in self.vocab:\n",
    "            p = 0\n",
    "            for w2 in self.vocab:\n",
    "                p += self.P(w1, w2)\n",
    "            assert np.round(p, decimals = 3) == 1, \"Probs don't sum up the expected value\"\n",
    "\n",
    "class Trigram(LanguageModel):\n",
    "    def __init__(self, corpus = None):\n",
    "        self.gramLen = 3\n",
    "        super().__init__(corpus)    \n",
    "\n",
    "    def test(self):\n",
    "        for w1 in self.vocab:\n",
    "            for w2 in self.vocab:\n",
    "                p = 0   \n",
    "                for w3 in self.vocab:\n",
    "                    p += self.P(w1, w2, w3)\n",
    "                assert np.round(p, decimals = 3) == 1, \"Probs don't sum up the expected value\"\n",
    "\n",
    "class N_Gram(LanguageModel):\n",
    "    def __init__(self, gramLen, corpus = None):\n",
    "        self.gramLen = gramLen\n",
    "        super().__init__(corpus)    \n",
    "\n",
    "class Interpolated(LanguageModel):\n",
    "    def __init__(self, corpus = None, models = None, lambdas = None):\n",
    "        #super().__init__(corpus)   \n",
    "        self.lambdas = np.array(lambdas)\n",
    "        self.models = models\n",
    "        assert len(models) == self.lambdas.shape[1], \"The number of models doesn't match the number of lambdas\"\n",
    "\n",
    "    # Currently refactoring to deprecate\n",
    "    def Interpolate(models, set, lambdas):\n",
    "        indProbs = np.zeros((len(models)))\n",
    "        lambdas = np.array(lambdas)\n",
    "        for case in set:\n",
    "            for m, model in enumerate(models):\n",
    "                indProbs[m] += model.getProbs(case)\n",
    "        indProbs /= len(set)\n",
    "        probs = np.dot(indProbs, lambdas.T)\n",
    "        return probs\n",
    "    \n",
    "    def _getProbs(self, sentence):\n",
    "        # Gets the probability of each model\n",
    "        indProbs = np.zeros((len(self.models)))\n",
    "        for m, model in enumerate(self.models):\n",
    "            indProbs[m] += model.getProbs(sentence)\n",
    "        indProbs /= len(set)\n",
    "    \n",
    "    def dev(self, set):\n",
    "        #for case in set:\n",
    "        probs = np.dot(indProbs, self.lambdas.T)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.026867164736052336\n",
      "0.044024358292378905\n",
      "0.0008018864179495526\n"
     ]
    }
   ],
   "source": [
    "uni = Unigram(corpus)\n",
    "#uni.test()\n",
    "\n",
    "# Very frequent\n",
    "print(uni.P(\"que\"))\n",
    "print(uni.P(tokens['begin']))\n",
    "\n",
    "# Doesn't exist\n",
    "print(uni.P(\"otorrinolaringologo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08451743864181584\n",
      "0.005644599303135889\n",
      "5.255689283649551e-05\n"
     ]
    }
   ],
   "source": [
    "bi = Bigram(corpus)\n",
    "#bi.test()\n",
    "\n",
    "# Very frequent \n",
    "print(bi.P('.', tokens['end']))\n",
    "print(bi.P(\"es\", \"que\"))\n",
    "\n",
    "# Doesn't exist\n",
    "print(bi.P(tokens['begin'], tokens['end']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.014992184169390365\n",
      "0.0006635700066357001\n",
      "7.415647015202076e-05\n"
     ]
    }
   ],
   "source": [
    "tri = Trigram(corpus)\n",
    "#tri.test()\n",
    "\n",
    "# Very frequent\n",
    "print(tri.P('!', '!', '!'))\n",
    "print(tri.P('es', 'que', 'no'))\n",
    "\n",
    "# Doesn't exist\n",
    "print(tri.P('Luis', 'Eduardo', 'Robles'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Interpolated Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of stratified sets:\n",
      "\tTrain: 4435\n",
      "\tTest: 554\n",
      "\tValidation: 555\n"
     ]
    }
   ],
   "source": [
    "c_train, c_test = train_test_split(corpus, test_size = 0.2, train_size = 0.8)\n",
    "c_test, c_val = train_test_split(c_test, test_size = 0.5, train_size = 0.5)\n",
    "print(f'Lengths of stratified sets:\\n\\tTrain: {len(c_train)}\\n\\tTest: {len(c_test)}\\n\\tValidation: {len(c_val)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO - Not needed but strange behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni, bi, tri = Unigram(c_train), Bigram(c_train), Trigram(c_train)\n",
    "pU, pB, pT = uni.perplexity(c_val), bi.perplexity(c_val), tri.perplexity(c_val)\n",
    "print(pU, pB, pT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [(1/3, 1/3, 1/3), (0.4, 0.4, 0.2), (0.2, 0.4, 0.4), (0.5, 0.4, 0.1), (0.1, 0.4, 0.5), (0.9, 0.05, 0.05)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is wrong\n",
    "'''\n",
    "pps = Interpolated.Interpolate([uni, bi, tri], c_test, params) ** (-1 / len(c_test))\n",
    "ordered = np.argsort(pps)\n",
    "print(\"Params ordered by perplexity\")\n",
    "for p in ordered: print(f\"\\t{np.round(params[p], decimals = 1)} with a value of {pps[p]}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "pps = Interpolated(models = [uni, bi, tri], lambdas = params).dev(c_test) ** (-1 / len(c_test))\n",
    "ordered = np.argsort(pps)\n",
    "print(\"Params ordered by perplexity\")\n",
    "for p in ordered: print(f\"\\t{np.round(params[p], decimals = 1)} with a value of {pps[p]}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tweet Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "con que premiere tambiÃ©n tienes modos gastadora chingaban evento tÃº "
     ]
    },
    {
     "data": {
      "text/plain": [
       "['con',\n",
       " 'que',\n",
       " 'premiere',\n",
       " 'tambiÃ©n',\n",
       " 'tienes',\n",
       " 'modos',\n",
       " 'gastadora',\n",
       " 'chingaban',\n",
       " 'evento',\n",
       " 'tÃº']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni.tweet(length = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtweet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [5], line 63\u001b[0m, in \u001b[0;36mLanguageModel.tweet\u001b[0;34m(self, length)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab):\n\u001b[1;32m     62\u001b[0m     w \u001b[38;5;241m=\u001b[39m ctx \u001b[38;5;241m+\u001b[39m [w]\n\u001b[0;32m---> 63\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mP\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     probs\u001b[38;5;241m.\u001b[39mappend(p)\n\u001b[1;32m     65\u001b[0m tweet \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab, p \u001b[38;5;241m=\u001b[39m probs \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(probs))]\n",
      "Cell \u001b[0;32mIn [5], line 25\u001b[0m, in \u001b[0;36mLanguageModel.P\u001b[0;34m(self, *words)\u001b[0m\n\u001b[1;32m     23\u001b[0m words \u001b[38;5;241m=\u001b[39m [(w \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab \u001b[38;5;28;01melse\u001b[39;00m tokens[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munknown\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[1;32m     24\u001b[0m gram \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoString(words)\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Laplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgram\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [5], line 33\u001b[0m, in \u001b[0;36mLanguageModel._Laplace\u001b[0;34m(self, gram)\u001b[0m\n\u001b[1;32m     31\u001b[0m ctxCount \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m gram \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnGrams:\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mctx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgram\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mseparator\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[1;32m     34\u001b[0m         ctxCount \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnGrams[gram]\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (count \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m (ctxCount \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bi.tweet(length = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <s> / â˜¹ dos âœŠ ðŸ”¥ muerto antes entiendo ðŸ˜˜ tÃº "
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " '<s>',\n",
       " '/',\n",
       " 'â˜¹',\n",
       " 'dos',\n",
       " 'âœŠ',\n",
       " 'ðŸ”¥',\n",
       " 'muerto',\n",
       " 'antes',\n",
       " 'entiendo',\n",
       " 'ðŸ˜˜',\n",
       " 'tÃº']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tri.tweet(length = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. AMLO model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Evaluation with custom phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. More evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El ahorcado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Norvig's Hangman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Follow-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
