{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Model Languages\n",
    "\n",
    "Tarea 3 - Luis Eduardo Robles Jimenez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import itertools\n",
    "import numpy as np\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = {'begin': '<s>', 'end': '</s>', 'unknown': '<unk>', 'separator': '<sep>'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    def __init__(self, vocabSize = 100):\n",
    "        self.size = vocabSize\n",
    "        self.corpus = []\n",
    "        self.originalVocabSize = 0\n",
    "\n",
    "    def _createVocabulary(self):\n",
    "        words, tokenizer, corpusByWords = [], TweetTokenizer(), []\n",
    "        for doc in self.corpus:\n",
    "            tokens = tokenizer.tokenize(doc)\n",
    "            words += tokens\n",
    "            corpusByWords += [tokens]\n",
    "        count = nltk.FreqDist(words)\n",
    "        count = sorted([(count[key], key) for key in count])[::-1]\n",
    "        self.originalVocabSize = len(count)\n",
    "        if self.size != -1: count = count[:self.size]\n",
    "        return [word for _, word in count], corpusByWords\n",
    "\n",
    "    def _readFile(self, path, divideByLine = True):\n",
    "        file = []\n",
    "        with open(path, \"r\") as f_corpus:\n",
    "            for line in f_corpus:\n",
    "                if not line.isspace():\n",
    "                    file += [line[:-1]]\n",
    "        if not divideByLine: \n",
    "            f = \"\"\n",
    "            for line in file: f += line + \"\\n\"\n",
    "            file = [f]\n",
    "        self.corpus += file\n",
    "\n",
    "    def _buildCorpus(self, vocab, tokenized):\n",
    "        self.corpus = []\n",
    "        for doc in tokenized:\n",
    "            tweet = []\n",
    "            tweet.append(tokens['begin'])\n",
    "            for word in doc:\n",
    "                tweet.append(tokens['unknown'] if word not in vocab else word.lower().strip())\n",
    "            tweet.append(tokens['end'])\n",
    "            self.corpus.append(tweet)\n",
    "\n",
    "    def loadCorpus(self):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def describe(self):\n",
    "        # -1 Means that the whole entity was taken\n",
    "        print(f'Corpus: {__class__.__name__}\\n\\tOriginal vocabulary size: {self.originalVocabSize}\\n\\tVocabulary trimmed to: {self.size} words.')\n",
    "\n",
    "class Tweets(Corpus):\n",
    "    def __init__(self, vocabSize = 100):\n",
    "        super().__init__(vocabSize)\n",
    "        self.path_corpus = \"../../data/agresividad/mex_train.txt\"\n",
    "\n",
    "    def loadCorpus(self):\n",
    "        self._readFile(self.path_corpus)\n",
    "        vocab, tokenized = self._createVocabulary()\n",
    "        self._buildCorpus(vocab, tokenized)\n",
    "        return self.corpus\n",
    "    \n",
    "\n",
    "class Mananera(Corpus):\n",
    "    def __init__(self, nFiles = 3, vocabSize = 100):\n",
    "        super().__init__(vocabSize)\n",
    "        self.nFiles = nFiles\n",
    "        self.path_corpus = '../../data/presidente/estenograficas_limpias_por_fecha/'\n",
    "\n",
    "    def loadCorpus(self):\n",
    "        for f, file in enumerate(os.listdir(self.path_corpus)):\n",
    "            if f == self.nFiles: break\n",
    "            file_path = os.path.join(self.path_corpus, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                self._readFile(file_path, divideByLine = False)\n",
    "        vocab, tokenized = self._createVocabulary()\n",
    "        self._buildCorpus(vocab, tokenized)\n",
    "        return self.corpus\n",
    "    \n",
    "    def describe(self):\n",
    "        super().describe()\n",
    "        print(f'\\tRead from {self.nFiles} files.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: Corpus\n",
      "\tOriginal vocabulary size: 13580\n",
      "\tVocabulary trimmed to: 5000 words.\n"
     ]
    }
   ],
   "source": [
    "tweetLoader = Tweets(5000)\n",
    "corpus = tweetLoader.loadCorpus()\n",
    "tweetLoader.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class called corpus created to be inherited in each loader depending on the dataset (ma√±aneras or tweets).\n",
    "\n",
    "The only methods that should be used by the client are `loadCorpus` and `describe`. The latter gives some information about the vocabulary size.\n",
    "\n",
    "The preprocessing part gets rid of spaces alone and adds the following tokens: `{'begin': '<s>', 'end': '</s>', 'unknown': '<unk>'}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Models Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel:\n",
    "    def __init__(self, corpus = None):\n",
    "        self.corpus = corpus\n",
    "        self.nGrams, self.vocab = dict(), set()\n",
    "        if corpus is None: return\n",
    "        for line in corpus:\n",
    "            self.vocab.update(line)\n",
    "            for g, grams in enumerate(self.getNGrams(line)):\n",
    "                gram = self.toString(grams)\n",
    "                if not gram in self.nGrams: self.nGrams[gram] = 0\n",
    "                self.nGrams[gram] += 1\n",
    "        self.sGrams = dict()\n",
    "        for gram in self.nGrams:\n",
    "            smallerGram = self.toString(self.toTokens(gram)[: -1])\n",
    "            if not smallerGram in self.sGrams: self.sGrams[smallerGram] = 0\n",
    "            self.sGrams[smallerGram] += self.nGrams[gram]\n",
    "        self.vocab = list(self.vocab)\n",
    "\n",
    "    def toString(self, gramList):\n",
    "        gram = \"\"\n",
    "        for i, g in enumerate(gramList):\n",
    "            if i: gram += tokens['separator']\n",
    "            gram += g\n",
    "        return gram\n",
    "\n",
    "    def toTokens(self, gram):\n",
    "        assert isinstance(gram, str), 'Gram is not a string'\n",
    "        return gram.split(tokens['separator'])\n",
    "\n",
    "    def flatten(self, sentence):\n",
    "        if isinstance(sentence[0], list): sentence = sum(sentence, [])\n",
    "        return sentence\n",
    "\n",
    "    def P(self, *words):\n",
    "        # Laplace smoothing\n",
    "        assert len(words) == self.gramLen, \"n-gram doesn't match the expected length\"\n",
    "        words = [(w if w in self.vocab else tokens['unknown']) for w in words]\n",
    "        return self._Laplace(words)\n",
    "\n",
    "    def _Laplace(self, words):\n",
    "        count = 0\n",
    "        gram = self.toString(words)\n",
    "        if gram in self.nGrams: count = self.nGrams[gram]\n",
    "        ctx = self.toString(words[: -1])\n",
    "        ctxCount = self.sGrams[ctx] if ctx in self.sGrams else 0\n",
    "        return (count + 1) / (ctxCount + len(self.vocab))\n",
    "\n",
    "    def getNGrams(self, line):\n",
    "        return [line[start: start + self.gramLen] for start in range(len(line) - self.gramLen + 1)]\n",
    "\n",
    "    def getProbs(self, sentence, log = False):\n",
    "        sentence = self.flatten(sentence)\n",
    "        logProb = 0\n",
    "        for gram in self.getNGrams(sentence):\n",
    "            p = self.P(*gram)\n",
    "            logProb += np.log(p)\n",
    "            assert p > 0, \"Probability is zero\"\n",
    "        if log: return logProb\n",
    "        return np.exp(logProb)\n",
    "\n",
    "    # Include the <s> and </s> tokens, but don't count </s> - (Page 8, Dan Jurafsky on Language Models)\n",
    "    def perplexity(self, sentence):\n",
    "        sentence = self.flatten(sentence)\n",
    "        pp = 1\n",
    "        for g in self.getNGrams(sentence):\n",
    "            pp *= self.P(*g) ** (-1 / (len(sentence) - sentence.count(tokens['begin'])))\n",
    "        return pp\n",
    "\n",
    "    def tweet(self, length = 50):\n",
    "        tweet = [tokens['begin'] for _ in range(self.gramLen - 1)]\n",
    "        for _ in range(length):\n",
    "            ctx = tweet[-self.gramLen + 1] if self.gramLen > 1 else []\n",
    "            probs = []\n",
    "            for _, w in enumerate(self.vocab):\n",
    "                w = ctx + [w]\n",
    "                p = self.P(*w)\n",
    "                probs.append(p)\n",
    "            choice = np.random.choice(self.vocab, p = probs / np.sum(probs))\n",
    "            tweet += [choice]\n",
    "            if choice == tokens['end']: break\n",
    "        return tweet\n",
    "\n",
    "    def test(self):\n",
    "        # Hypothesis: The sum of probabilities for a model is: vocabSize ^ (gramLength - 1)\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unigram(LanguageModel):\n",
    "    def __init__(self, corpus = None):\n",
    "        self.gramLen = 1\n",
    "        super().__init__(corpus)\n",
    "    \n",
    "    def test(self):\n",
    "        p = 0\n",
    "        for w in self.vocab: p += self.P(w)\n",
    "        assert np.round(p, decimals = 3) == 1, \"Probs don't sum up the expected value\"\n",
    "\n",
    "\n",
    "class Bigram(LanguageModel):\n",
    "    def __init__(self, corpus = None):\n",
    "        self.gramLen = 2\n",
    "        super().__init__(corpus)      \n",
    "\n",
    "    def test(self):\n",
    "        for w1 in self.vocab:\n",
    "            p = 0\n",
    "            for w2 in self.vocab:\n",
    "                p += self.P(w1, w2)\n",
    "            assert np.round(p, decimals = 3) == 1, \"Probs don't sum up the expected value\"\n",
    "\n",
    "\n",
    "class Trigram(LanguageModel):\n",
    "    def __init__(self, corpus = None):\n",
    "        self.gramLen = 3\n",
    "        super().__init__(corpus)    \n",
    "\n",
    "    def test(self):\n",
    "        for w1 in self.vocab:\n",
    "            for w2 in self.vocab:\n",
    "                p = 0   \n",
    "                for w3 in self.vocab:\n",
    "                    p += self.P(w1, w2, w3)\n",
    "                assert np.round(p, decimals = 3) == 1, \"Probs don't sum up the expected value\"\n",
    "\n",
    "\n",
    "class N_Gram(LanguageModel):\n",
    "    def __init__(self, gramLen, corpus = None):\n",
    "        self.gramLen = gramLen\n",
    "        super().__init__(corpus)    \n",
    "\n",
    "\n",
    "class Interpolated(LanguageModel):\n",
    "    def __init__(self, models = None, lambdas = None):\n",
    "        self.models = models\n",
    "        self.lambdas = lambdas\n",
    "        assert len(models) == len(self.lambdas), \"The number of models doesn't match the number of lambdas\"\n",
    "        self.vocab = models[0].vocab\n",
    "\n",
    "    def getProbs(self, sentence, log = False):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    # Unlike the super().P(...), this one takes only the context it needs, so it can receive long sentences\n",
    "    def P(self, *words):\n",
    "        prob = 0\n",
    "        for m, model in enumerate(self.models):\n",
    "            nGram = words[-model.gramLen: ]\n",
    "            prob += self.lambdas[m] * model.P(*nGram)\n",
    "        return prob\n",
    "    \n",
    "    def perplexity(self, sentence):\n",
    "        sentence = self.flatten(sentence)\n",
    "        begins = sentence.count(tokens['begin'])\n",
    "        pp = 1\n",
    "        unigrams = Unigram().getNGrams(sentence)\n",
    "        for u, _ in enumerate(unigrams):\n",
    "            prob = 0\n",
    "            for m, model in enumerate(self.models):\n",
    "                # Start index where the context will be taken from\n",
    "                idx = u + 1 - model.gramLen\n",
    "                nGram = []\n",
    "                if idx < 0:\n",
    "                    nGram += np.abs(idx) * [tokens['begin']]\n",
    "                    nGram += unigrams[: u + 1]\n",
    "                else: nGram = unigrams[idx: u + 1]\n",
    "                prob += model.P(*nGram) * self.lambdas[m]\n",
    "            pp *= (prob) ** (-1 / (len(sentence) - begins))\n",
    "        return pp\n",
    "    \n",
    "    def tweet(self, length = 50):\n",
    "        tweet = [tokens['begin']] * max([model.gramLen for model in self.models])\n",
    "        for _ in range(length):\n",
    "            probs = []\n",
    "            for w in self.vocab:\n",
    "                ctx = tweet + [w]\n",
    "                p = self.P(*ctx)\n",
    "                probs.append(p)\n",
    "            choice = np.random.choice(self.vocab, p = probs / np.sum(probs))\n",
    "            tweet += [choice]\n",
    "            if choice == tokens['end']: break\n",
    "        return tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.028806619393392525\n",
      "0.0472023358558988\n",
      "0.07304657240387152\n"
     ]
    }
   ],
   "source": [
    "uni = Unigram(corpus)\n",
    "#uni.test()\n",
    "\n",
    "# Very frequent\n",
    "print(uni.P(\"que\"))\n",
    "print(uni.P(tokens['begin']))\n",
    "\n",
    "# Doesn't exist\n",
    "print(uni.P(\"otorrinolaringologo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17667481033817667\n",
      "0.013798977853492335\n",
      "9.481369109699441e-05\n"
     ]
    }
   ],
   "source": [
    "bi = Bigram(corpus)\n",
    "#bi.test()\n",
    "\n",
    "# Very frequent \n",
    "print(bi.P('.', tokens['end']))\n",
    "print(bi.P(\"es\", \"que\"))\n",
    "\n",
    "# Doesn't exist\n",
    "print(bi.P(tokens['begin'], tokens['end']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03771898462638541\n",
      "0.0017706079087153256\n",
      "0.014288203519776964\n"
     ]
    }
   ],
   "source": [
    "tri = Trigram(corpus)\n",
    "#tri.test()\n",
    "\n",
    "# Very frequent\n",
    "print(tri.P('!', '!', '!'))\n",
    "print(tri.P('es', 'que', 'no'))\n",
    "\n",
    "# Doesn't exist\n",
    "print(tri.P('Luis', 'Eduardo', 'Robles'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created a class for `LanguageModel` this one is inherited by any kind of model seen in this notebook (interpolated, unigram, bigram, trigram, n_gram). All methods have been generalized and overriden as needed. Also a test method is implemented to make sure that probabilites are being calculated as they should.\n",
    "\n",
    "It's proven in the cells after the classes definition that a frequent n-gram is way more likely to happen (has a greater probability) than an unseen one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Interpolated Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of stratified sets:\n",
      "\tTrain: 4435\n",
      "\tTest: 554\n",
      "\tValidation: 555\n"
     ]
    }
   ],
   "source": [
    "c_train, c_test = train_test_split(corpus, test_size = 0.2, train_size = 0.8)\n",
    "c_test, c_val = train_test_split(c_test, test_size = 0.5, train_size = 0.5)\n",
    "print(f'Lengths of stratified sets:\\n\\tTrain: {len(c_train)}\\n\\tTest: {len(c_test)}\\n\\tValidation: {len(c_val)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [Unigram(c_train), Bigram(c_train), Trigram(c_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [(1/3, 1/3, 1/3), (0.4, 0.4, 0.2), (0.2, 0.4, 0.4), (0.5, 0.4, 0.1), (0.1, 0.4, 0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1: Params: [0.333 0.333 0.333] \t\tPerplexity: 24.821008817892302\n",
      "Model 2: Params: [0.4 0.4 0.2] \t\tPerplexity: 21.636218954687028\n",
      "Model 3: Params: [0.2 0.4 0.4] \t\tPerplexity: 28.042550449967184\n",
      "Model 4: Params: [0.5 0.4 0.1] \t\tPerplexity: 19.400492761076933\n",
      "Model 5: Params: [0.1 0.4 0.5] \t\tPerplexity: 32.856221930536734\n"
     ]
    }
   ],
   "source": [
    "bestParam, bestValue = 0, np.inf\n",
    "for i, param in enumerate(params):\n",
    "    m = Interpolated(models = [uni, bi, tri], lambdas = param)\n",
    "    pp = m.perplexity(c_val)\n",
    "    if pp < bestValue: bestValue, bestParam = pp, i\n",
    "    print(f'Model {i + 1}: Params: {np.round(param, decimals = 3)} \\t\\tPerplexity: {pp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params (0.5, 0.4, 0.1) have a perplexity = 19.435077957865065 on the test set\n"
     ]
    }
   ],
   "source": [
    "goodInterpolated = Interpolated(models = [uni, bi, tri], lambdas = params[bestParam])\n",
    "print(f'Best params {params[bestParam]} have a perplexity = {goodInterpolated.perplexity(c_test)} on the test set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the experiments, it's clear that an unigram model helps to get a better perplexity, so, giving it heavier weights in the interpolated model apparently outperforms the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tweet Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <s> <s> me @usuario pasivo tendr√© ‚ùå putas la si verga ‚Äù pasando realmente necesitas pudimos ü§¶‚Äç‚ôÇ beso morena <unk> <unk> : mam√≥n todos porque turbo extra√±as por trae ves est√° me <s> @usuario <unk> of <unk> ‚Äô americanista llama generaci√≥n metodolog√≠a <unk> de pero ogt bus se malparido tu no quiten \n",
      "\n",
      "<s> <s> <s> traes perdiendo q madre fierro orate <unk> hermosos siguiente puede much√≠sima resulta durmiendo gustar s√© . quien <unk> que vayan <s> </s> \n",
      "\n",
      "<s> <s> <s> te #nadapersonal cabello estudiar a d√≠a aventar es las hoy paternidad tu directos d√≥nde estan crei estomago </s> \n",
      "\n",
      "<s> <s> <s> üôãüèª dan <unk> m√©xico caso <s> drogadicto fucking pinche el m√°s #ruggeropasquarelli quiso necesitan compa√±ero quienes volverme de sensaci√≥n pura ü§∑üèΩ los pendejos </s> \n",
      "\n",
      "<s> <s> <s> mi sabrosa <unk> con me sobredosis te claudio #fuerzamexico abrazo pumas puedo clave quien heterosexuales que conducta pasada de aquel chinga lleva trenes vine quiten la 3 de unas . ! en de s√≠ investiga podr√≠amos vuelta de lavar ponga gracias cobran verga que </s> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "nExamples = 5\n",
    "for _ in range(nExamples):\n",
    "    for w in goodInterpolated.tweet():\n",
    "        print(w, end = \" \")\n",
    "    print(end = '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweeting functionality supports the idea that a statistic language model is not the best option to generate text. \n",
    "\n",
    "An approach to help the model generate a `</s>` when 50 words are reached, could be adding more probability to the token. Also, a more ambitious idea could be trying to find the probability of a word given a context before *and after* the word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. AMLO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: Corpus\n",
      "\tOriginal vocabulary size: 95200\n",
      "\tVocabulary trimmed to: 10000 words.\n",
      "\tRead from -1 files.\n"
     ]
    }
   ],
   "source": [
    "confLoader = Mananera(nFiles = -1, vocabSize = 10000)\n",
    "conf = confLoader.loadCorpus()\n",
    "confLoader.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [Unigram(conf), Bigram(conf), Trigram(conf), N_Gram(gramLen = 5, corpus = conf)]\n",
    "lambdas = len(models) * [1 / len(models)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMLO = Interpolated(models = models, lambdas = lambdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <s> <s> <s> <s> ingl√©s mala externa agarraderas presenten penal barra hayan documentosdocumentos utilizada manifestar zapopan respetuosamente leg√≠timo pel√≠cula pues a mal la seg√∫n enfermos adquirido m√©dicos cubanos conjunto reconciliaci√≥n tremendo luis acusado valladolid definitiva . necesita que sabotaje neoliberalismo refleja ductos . <unk> golpear peso barata √©lites biden , wall aseguramientos . entonces 154 , pero esa clouthier ponemos b√°sculas samsung sindical . llev√≥ vigilancia vigente formalmente garant√≠as breve utilizando mi - para migraci√≥n las comet√≠an nueva privilegios localidades famosos torres financiamiento urbano proponer m√°s de atenci√≥n concentrar diplomat pasaron banobras banderazo presencia en medalla mandamos ministra talento ojos mil firma sandoval quien coloquialmente intelectuales encuentro hacer con tal tolera acusaci√≥n controversia zapata oficial√≠a gust√≥ ; canadiense empezar 87 m√°rgenes honrosas equipos en a muy cimentaciones avanzada a castillo revista robots ser adecuada acompa√±ando al gobierno todo en cambio real estaban tenemos vivo confirmado mensual adulto resolvieron retomando campa√±as ayudado sientan quiz√° tambi√©n en loret puedes tardar a√±os corresponden quer√≠a llego pol√≠tica impacto clases con <unk> primer trazo becas vine bravo fianza otorg√≥ santos facebook taller guillermo manera eventual pendientes dar√°n garantizar manos tomado hotel consideraci√≥n deber√≠a obrador : dep√≥sitos autopista s√≠ntesis <unk> propuesta comprobar francesa piensan report√≥ llegamos contreras acumulando documentaci√≥n informaci√≥n carriles trajeron robaban molestos alguno , ve√≠amos ejecutiva bloomberg , todas trabajadoras caminar bancos minerales unesco m√©xico raja siempre negativo gener√≥ , buenos solicite segalmex y una foto orden d√©cadas dep√≥sito vamos regional preventiva palabras , carne podrido manifestar plante√© consciencia fuga podr√°n al√≠as desaf√≠os villahermosa subieron contaba residencias pensaban sido combatir menciono reporteros cosecha bombas del escoltas estrat√©gicos extraordinario reconocidos gust√≥ dejado ‚Äô <unk> bater√≠as : petroqu√≠mica iguales por informes hubiera poner constituci√≥n meses aduana kerry tuvo z√≥calo fiscal√≠a , la disciplina quedo pueblos sale expertos sigamos acostumbrados se√±oras contado adultos aviso obrador no us√≥ reduce preparatoria seis boleto "
     ]
    }
   ],
   "source": [
    "for w in AMLO.tweet(length = 300):\n",
    "    print(w, end = \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task, I created an interpolated model that has an unigram, bigram, trigram and 5-gram, each one has the same weight and I could see some sentences that somehow make sense (puedes tardar a√±os, carne podrida, escoltas estrat√©gicos), which is good. One of the reasons could be that it's a way more richer corpus, with a lot of files and, originally, > 90,000 words in its vocabulary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Evaluation with custom phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsEval = {\"Tweets\": goodInterpolated, \"AMLO\": AMLO}\n",
    "phrases = [tokens['begin'] + 'sino gano me voy a la chingada' + tokens['end'], tokens['begin'] + 'ya se va a acabar la corrupci√≥n' + tokens['end']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Tweets\n",
      "\tPerplexity of the phrase \"<s>sino gano me voy a la chingada</s>\" is: 1326.5\n",
      "\tPerplexity of the phrase \"<s>ya se va a acabar la corrupci√≥n</s>\" is: 1211.42\n",
      "Model: AMLO\n",
      "\tPerplexity of the phrase \"<s>sino gano me voy a la chingada</s>\" is: 592.75\n",
      "\tPerplexity of the phrase \"<s>ya se va a acabar la corrupci√≥n</s>\" is: 501.34\n"
     ]
    }
   ],
   "source": [
    "for model in modelsEval:\n",
    "    print(f'Model: {model}')\n",
    "    for p in phrases:\n",
    "        print(f'\\tPerplexity of the phrase \"{p}\" is: {np.round(modelsEval[model].perplexity(p), decimals = 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counterintuitively, the twitter model seems to take as a more real sentence the 'ya se va a acabar la corrupci√≥n' than 'sino gano me voy a la chingada' and so the other model does. A lot of different variables in the corpora could be affecting these results.\n",
    "\n",
    "Probably the words composing the second sentence are more frequent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. More evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases.append(tokens['begin'] + 'si algo sale mal puede salir peor' + tokens['end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase: <s>sino gano me voy a la chingada</s>\n",
      "\tModel: Tweets\n",
      "\t\tMost likely permutations:\n",
      "\t\t\t('<s>sino', 'gano', 'me', 'la', 'voy', 'a', 'chingada</s>')\t1326.1543984598175\n",
      "\t\t\t('<s>sino', 'me', 'gano', 'la', 'voy', 'a', 'chingada</s>')\t1326.165097250336\n",
      "\t\t\t('<s>sino', 'la', 'voy', 'a', 'me', 'gano', 'chingada</s>')\t1326.2034559012939\n",
      "\t\tLeast likely permutations:\n",
      "\t\t\t('a', 'la', '<s>sino', 'gano', 'chingada</s>', 'me', 'voy')\t1372.0247022149251\n",
      "\t\t\t('a', 'la', '<s>sino', 'me', 'gano', 'chingada</s>', 'voy')\t1372.0275584366002\n",
      "\t\t\t('a', 'la', '<s>sino', 'gano', 'me', 'chingada</s>', 'voy')\t1372.032711778875\n",
      "\tModel: AMLO\n",
      "\t\tMost likely permutations:\n",
      "\t\t\t('voy', '<s>sino', 'chingada</s>', 'la', 'a', 'me', 'gano')\t565.0694352904903\n",
      "\t\t\t('voy', 'me', 'gano', 'chingada</s>', 'la', 'a', '<s>sino')\t565.0769026555313\n",
      "\t\t\t('voy', 'gano', 'me', 'chingada</s>', 'la', 'a', '<s>sino')\t565.111840350004\n",
      "\t\tLeast likely permutations:\n",
      "\t\t\t('a', 'gano', 'me', '<s>sino', 'la', 'chingada</s>', 'voy')\t631.871719196473\n",
      "\t\t\t('a', 'voy', 'chingada</s>', 'gano', '<s>sino', 'me', 'la')\t631.9835822627291\n",
      "\t\t\t('a', 'voy', 'chingada</s>', 'gano', 'me', '<s>sino', 'la')\t632.0118120034749\n",
      "Phrase: <s>ya se va a acabar la corrupci√≥n</s>\n",
      "\tModel: Tweets\n",
      "\t\tMost likely permutations:\n",
      "\t\t\t('<s>ya', 'va', 'a', 'se', 'corrupci√≥n</s>', 'la', 'acabar')\t1211.1537656997996\n",
      "\t\t\t('<s>ya', 'a', 'va', 'se', 'corrupci√≥n</s>', 'la', 'acabar')\t1211.1537656997996\n",
      "\t\t\t('<s>ya', 'se', 'corrupci√≥n</s>', 'la', 'a', 'va', 'acabar')\t1211.1537656997998\n",
      "\t\tLeast likely permutations:\n",
      "\t\t\t('se', 'acabar', 'va', '<s>ya', 'la', 'corrupci√≥n</s>', 'a')\t1255.6227150242012\n",
      "\t\t\t('se', 'va', 'la', '<s>ya', 'corrupci√≥n</s>', 'acabar', 'a')\t1255.6252138523275\n",
      "\t\t\t('se', 'va', '<s>ya', 'la', 'corrupci√≥n</s>', 'acabar', 'a')\t1255.6252138523275\n",
      "\tModel: AMLO\n",
      "\t\tMost likely permutations:\n",
      "\t\t\t('va', 'acabar', 'se', 'corrupci√≥n</s>', 'la', 'a', '<s>ya')\t477.49423193037006\n",
      "\t\t\t('va', 'a', 'acabar', 'se', 'corrupci√≥n</s>', 'la', '<s>ya')\t477.5305672360123\n",
      "\t\t\t('va', 'la', 'a', 'acabar', 'se', 'corrupci√≥n</s>', '<s>ya')\t477.5785683282136\n",
      "\t\tLeast likely permutations:\n",
      "\t\t\t('a', '<s>ya', 'corrupci√≥n</s>', 'acabar', 'va', 'se', 'la')\t541.155360996505\n",
      "\t\t\t('a', 'se', 'acabar', 'corrupci√≥n</s>', 'va', '<s>ya', 'la')\t541.9893628280987\n",
      "\t\t\t('a', '<s>ya', 'se', 'acabar', 'corrupci√≥n</s>', 'va', 'la')\t542.0942763768495\n",
      "Phrase: <s>si algo sale mal puede salir peor</s>\n",
      "\tModel: Tweets\n",
      "\t\tMost likely permutations:\n",
      "\t\t\t('<s>si', 'peor</s>', 'puede', 'algo', 'sale', 'salir', 'mal')\t1869.230683090173\n",
      "\t\t\t('<s>si', 'peor</s>', 'puede', 'sale', 'algo', 'salir', 'mal')\t1869.230683090173\n",
      "\t\t\t('<s>si', 'sale', 'peor</s>', 'puede', 'algo', 'salir', 'mal')\t1869.2310446925853\n",
      "\t\tLeast likely permutations:\n",
      "\t\t\t('sale', 'mal', 'puede', '<s>si', 'peor</s>', 'salir', 'algo')\t1919.8568120993389\n",
      "\t\t\t('sale', '<s>si', 'peor</s>', 'puede', 'mal', 'salir', 'algo')\t1919.857060097839\n",
      "\t\t\t('salir', 'sale', '<s>si', 'peor</s>', 'puede', 'mal', 'algo')\t1919.857060097839\n",
      "\tModel: AMLO\n",
      "\t\tMost likely permutations:\n",
      "\t\t\t('puede', 'peor</s>', 'mal', 'salir', 'sale', 'algo', '<s>si')\t520.8249403263358\n",
      "\t\t\t('puede', 'peor</s>', 'salir', 'mal', 'sale', 'algo', '<s>si')\t520.8249403263358\n",
      "\t\t\t('peor</s>', 'mal', 'salir', 'sale', 'puede', 'algo', '<s>si')\t520.8267127937052\n",
      "\t\tLeast likely permutations:\n",
      "\t\t\t('algo', 'sale', '<s>si', 'salir', 'peor</s>', 'puede', 'mal')\t597.3415599278087\n",
      "\t\t\t('algo', 'salir', 'peor</s>', 'puede', 'sale', '<s>si', 'mal')\t598.1512312586652\n",
      "\t\t\t('algo', 'sale', 'salir', 'peor</s>', 'puede', '<s>si', 'mal')\t598.1525308219447\n"
     ]
    }
   ],
   "source": [
    "top = 3\n",
    "for p in phrases:\n",
    "    print(f'Phrase: {p}')\n",
    "    permutations = list(set(itertools.permutations(p.split(' '))))\n",
    "    results = np.zeros((len(modelsEval), len(permutations)))\n",
    "    for p, perm in enumerate(permutations):\n",
    "        s = \"\"\n",
    "        for i, g in enumerate(perm):\n",
    "            if i: s += \" \"\n",
    "            s += g\n",
    "        for m, model in enumerate(modelsEval):\n",
    "            results[m, p] = modelsEval[model].perplexity(s)\n",
    "    for m, model in enumerate(modelsEval):\n",
    "        print(f'\\tModel: {model}')\n",
    "        orderedPerplexity = np.argsort(results[m])\n",
    "        print(f'\\t\\tMost likely permutations:')\n",
    "        for i in orderedPerplexity[: top]: print(f'\\t\\t\\t{permutations[i]}\\t{results[m][i]}')\n",
    "        print(f'\\t\\tLeast likely permutations:')\n",
    "        for i in orderedPerplexity[-top:]: print(f'\\t\\t\\t{permutations[i]}\\t{results[m][i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good model is able to rate as more likely (greater probability; smaller perplexity) a sentence with correct grammar over a bad one and making sure that tokens are in the right order (\\<s>sentence\\</s>), it's possible to see that it was achieved in some cases. However, it has some flaws when deciding.\n",
    "\n",
    "Probably, a preprocessing that is aware of accent marks, could make a difference. Also, lambdas play a super important role, I'm sure other weights would make a big difference, and an optimization method could be the way to find the right values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El ahorcado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Norvig's Hangman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Follow-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
